{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[{"file_id":"1wTFFEFF-nikaOONcOC-i6Ck_f9Ozn7dB","timestamp":1623381607498}],"authorship_tag":"ABX9TyOjaYqQ0KqYyTzgxVkyQBSn"},"accelerator":"GPU","kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### MMDetection 설치\n* 강의 영상에는 pip install mmcv-full로 mmcv를 설치(약 10분 정도의 시간이 소요)\n* 실습코드는 pip install mmcv-full -f https://download.openmmlab.com/mmcv/dist/cu116/torch1.13/index.html 로 변경(설치에 12초 정도 걸림. 2022.09).\n*  2023년 4월 6일 기준으로 mmdetection이 3.0으로 upgrade됨. 실습 코드는 mmdetection 2.x 기준으므로 mmdetection 2.x 소스코드 설치 필요.\n* 2024년 9월 colab의 numpy version이 1.24로 upgrade되면서 일부 코드가 동작오류. numpy 1.23 으로 downgrade 적용\n* 2025년 1월 17일 Colab의 python 버전이 3.10에서 3.11로 버전업 되면서 pytorch 2.0, torchvision 0.15로 변경. mmcv도 !pip install mmcv-full -f https://download.openmmlab.com/mmcv/dist/cu118/torch2.0.0/index.html 로 변경.\n* 2025년 8월 25일 Colab의 python 버전이 3.11에서 3.12로 버전업 되면서 더 이상 mmcv-full이 제대로 설치 되지 않음.\n* 이에 Colab 환경에서 Kaggle 환경으로 실습환경 이관. Kaggle은 여전히 python 버전이 3.11임.\n* 기존 Colab의 디렉토리 구조는 /content 디렉토리를 기반으로 실습코드 수행됨. Kaggle에서는 /kaggle/working이며 자동으로 현재디렉토리(.)로 실습 코드를 변경함\n* 2025년 8월 25일 download.openmmlab.com 사이트의 ssl 이슈로 pip install 에 --trusted-host 옵션 및 wget에 --no-check-certificate 옵션 추가","metadata":{"id":"TRvHk7ZOQb3D"}},{"cell_type":"markdown","source":"#### pytorch, torchvision 다운그레이드","metadata":{"id":"F_nNu2bWWg2N"}},{"cell_type":"code","source":"#코랩의 pytorch 버전이 2.x 로 upgrade\nimport torch\nprint(torch.__version__)","metadata":{"id":"bVLFyU4_bmoy","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# pytorch 버전을 2.0으로 downgrade\n!pip install torch==2.0.0 torchvision==0.15.1 --index-url https://download.pytorch.org/whl/cu118","metadata":{"id":"DpsE9kPJV4eD","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### mmcv-full 설치\n* mmcv-full은 1.7.2 버전으로 설치. ssl 인증 이슈로 --trusted-host 옵션 추가","metadata":{"id":"13f48TebWm_X"}},{"cell_type":"code","source":"!pip install mmcv-full --trusted-host download.openmmlab.com -f https://download.openmmlab.com/mmcv/dist/cu118/torch2.0/index.html","metadata":{"id":"RQtJqgDTc-AU","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### MMDetection 2.x 버전 설치\n* 2023년 4월 6일 기준으로 mmdetection이 3.0으로 upgrade됨. 실습 코드는 mmdetection 2.x 기준임.\n* mmdetection 2.x branch의 소스코드 기반으로 mmdetection 설치 필요.","metadata":{}},{"cell_type":"code","source":"# 2023년 4월 6일 기준으로 mmdetection이 3.0으로 upgrade됨. 실습 코드는 mmdetection 2.x 기준임.\n# mmdetection 2.x branch의 소스코드 기반으로 mmdetection 설치 필요.\n!git clone --branch 2.x https://github.com/open-mmlab/mmdetection.git\n!cd mmdetection; python setup.py install","metadata":{"id":"2nTvh01pc-M5","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### mmdetection 2.x 버전에서 numpy 호환성 이슈로 numpy downgrade\n* 반드시 numpy downgrade를 mmcv 설치 후에 실행할것.","metadata":{}},{"cell_type":"code","source":"### 반드시 numpy downgrade를 mmcv 설치 후에 실행할것.\n!pip install numpy==1.23","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### mmcv와 mmdetection이 제대로 설치되었는지 확인 ","metadata":{}},{"cell_type":"code","source":"# 아래를 수행하기 전에 kernel을 restart 해야 함.\nfrom mmdet.apis import init_detector, inference_detector\nimport mmcv","metadata":{"id":"mlIOlLLLjAP7","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\n\nprint(f\"Setup complete. Using torch {torch.__version__} ({torch.cuda.get_device_properties(0).name if torch.cuda.is_available() else 'CPU'})\")","metadata":{"id":"t_dDdanyVrJ2","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### MS-COCO 데이터 기반으로 Mask RCNN Pretrained 모델을 활용하여 Inference 수행\n* Mask RCNN Pretrained 모델 다운로드\n* Mask RCNN용 Config 파일 설정.  \n* Inference 용 모델을 생성하고, Inference 적용\n* 강의 영상은 colab 기준. /content 디렉토리를 kaggle 기준으로 현재 디렉토리 /kaggle/working 로 전환\n* download.openmmlab.com 사이트의 ssl 이슈로 wget에 --no-check-certificate 추가","metadata":{"id":"qHm73mVCQmpc"}},{"cell_type":"code","source":"# pretrained weight 모델을 다운로드 받기 위해서 mmdetection/checkpoints 디렉토리를 만듬.\n!cd mmdetection; mkdir checkpoints","metadata":{"id":"Gs162gTppmTh","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!wget --no-check-certificate -O /kaggle/working/mmdetection/checkpoints/mask_rcnn_r101_fpn_1x_coco_20200204-1efe0ed5.pth http://download.openmmlab.com/mmdetection/v2.0/mask_rcnn/mask_rcnn_r101_fpn_1x_coco/mask_rcnn_r101_fpn_1x_coco_20200204-1efe0ed5.pth","metadata":{"id":"BvBLA2ecqJsV","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!ls -lia /kaggle/working/mmdetection/checkpoints","metadata":{"id":"YbETJ0zYh_bf","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# config 파일을 설정하고, 다운로드 받은 pretrained 모델을 checkpoint로 설정.\nconfig_file = '/kaggle/working/mmdetection/configs/mask_rcnn/mask_rcnn_r101_fpn_1x_coco.py'\ncheckpoint_file = '/kaggle/working/mmdetection/checkpoints/mask_rcnn_r101_fpn_1x_coco_20200204-1efe0ed5.pth'","metadata":{"id":"DHyGQYXpoM_u","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# config 파일과 pretrained 모델을 기반으로 Detector 모델을 생성.\nfrom mmdet.apis import init_detector, inference_detector\n\nmodel = init_detector(config_file, checkpoint_file, device='cuda:0')","metadata":{"id":"BUgqp_sOpPKN","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import cv2\nimport matplotlib.pyplot as plt\nimg = '/kaggle/working/mmdetection/demo/demo.jpg'\n\nimg_arr  = cv2.imread(img)\nimg_arr_rgb = cv2.cvtColor(img_arr, cv2.COLOR_BGR2RGB)\nplt.figure(figsize=(12, 12))\nplt.imshow(img_arr_rgb)","metadata":{"id":"5DwPX9OslRgc","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"img_path = '/kaggle/working/mmdetection/demo/demo.jpg'\n# inference_detector의 인자로 string(file경로), ndarray가 단일 또는 list형태로 입력 될 수 있음.\nresults = inference_detector(model, img_arr)","metadata":{"id":"1U7BfoXrpR4l","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from mmdet.apis import show_result_pyplot\n# inference 된 결과를 원본 이미지에 적용하여 새로운 image로 생성(bbox 처리된 image)\n# Default로 score threshold가 0.3 이상인 Object들만 시각화 적용. show_result_pyplot은 model.show_result()를 호출.\nshow_result_pyplot(model, img_arr, results)","metadata":{"id":"KAdCpXVXQC3S","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Inference 결과로 반환되는 results 값 살펴 보기\n* inference_detector(model, img_path)의 결과로 반환되는 값은 instance segmentation 일 경우 Object Detection 결과에 추가되어 segmentation masking 정보도 함께 반환됨","metadata":{"id":"yXcqjkK2jdHA"}},{"cell_type":"code","source":"type(results), len(results)","metadata":{"id":"dkWvvhR6qvcZ","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"results","metadata":{"id":"U9pNu_6OfenS","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(type(results[0]), len(results[0]), type(results[1]), len(results[1]) )","metadata":{"id":"mGjvF2pNQVJI","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"'''results[0]는 list형으로 coco class의  0부터 79까지 class_id별로 80개의 array를 가짐.\n개별 array들은 각 클래스별로 5개의 값(좌표값과 class별로 confidence)을 가짐. 개별 class별로 여러개의 좌표를 가지면 여러개의 array가 생성됨.\n좌표는 좌상단(xmin, ymin), 우하단(xmax, ymax) 기준.\n개별 array의 shape는 (Detection된 object들의 수, 5(좌표와 confidence)) 임\n'''\nprint(type(results[0][0]), results[0][0].shape, results[0][1].shape, results[0][2].shape)","metadata":{"id":"FdRdB45NQqG1","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# results[1]은 masking 정보를 가짐. coco class의  0부터 79까지 class_id 별로 80개의 list를 가짐. 개별 list는 개별 object의 mask 정보를 내부 원소로 가짐.\n# 개별 object의 mask 정보는 2차원 array로서 image의 height x width 형태를 가짐.\nprint('results[1]의 첫번째 원소의 type과 size:', type(results[1][0]), len(results[1][0]), '두번째 원소 사이즈:', len(results[1][1]), '세번째 원소 사이즈:', len(results[1][2]))\nprint('results[1]의 첫번째 원소 list의 첫번째 원소 type과 shape:', type(results[1][0][0]), results[1][0][0].shape)\n#print('results[1]의 두번째 원소 list의 첫번째 원소 type과 shape:', type(results[1][1][0]))\nprint('results[1]의 세번째 원소 list의 첫번째/두번째/세번째 원소 shape:', results[1][2][0].shape, results[1][2][1].shape, results[1][2][2].shape)\nprint('image shape:', img_arr.shape)","metadata":{"id":"Ou9DI1ayQIvr","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"mask_imsi = results[1][0][0]\nprint(mask_imsi)\nprint(mask_imsi[mask_imsi > 0], mask_imsi[mask_imsi == 0])\n","metadata":{"id":"2HM7y-_dS1Yq","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"mask_imsi.shape","metadata":{"id":"sN-7fTm8k4jj","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\n\nmask_index = np.where(mask_imsi > 0)","metadata":{"id":"LL17XN52k9ar","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"np.array(mask_index)","metadata":{"id":"I6vBuTi6laQX","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"img_arr_rgb_copy = img_arr_rgb.copy()\nimg_arr_rgb_copy[mask_index[0], mask_index[1], :] =0\n","metadata":{"id":"R59_Vi9rlqXm","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(12, 12))\nplt.imshow(img_arr_rgb_copy)","metadata":{"id":"KIwwL6-2l9US","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\n\ndef apply_mask(image, mask, color, alpha=0.5):\n  for c in range(3):\n    # mask값이 1일 경우는 원본 pixel값에 컬러 segmentation을 적용하고, 그렇지 않을 경우 원본 pixel값을 그대로 유지.\n    image[:, :, c] = np.where(mask == 1,\n                              image[:, :, c] *\n                              (1 - alpha) + alpha * color[c] * 255,\n                              image[:, :, c])\n  return image\n\n\ndraw_img = cv2.cvtColor(img_arr, cv2.COLOR_BGR2RGB)\nmasked_image = apply_mask(draw_img, results[1][2][0], (0, 255, 0), alpha=0.6)\nplt.figure(figsize=(12, 14))\nplt.imshow(masked_image)\nplt.axis('off')","metadata":{"id":"bipE2MKLOa_b","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Video Detection 실행\n* mmdetection의 model.show_result()를 실행하여 frame별로 object detect 된 결과를 시각화","metadata":{"id":"trgaz_nOrfl2"}},{"cell_type":"code","source":"!mkdir data","metadata":{"id":"vJDIEqGwq9wu","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!wget -O /kaggle/working/data/John_Wick_small.mp4 https://github.com/chulminkw/DLCV/blob/master/data/video/John_Wick_small.mp4?raw=true","metadata":{"id":"688qOoVMKH9u","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# https://github.com/open-mmlab/mmdetection/blob/master/demo/video_demo.py 대로 video detection 수행.\nimport cv2\n\nvideo_reader = mmcv.VideoReader('/kaggle/working/data/John_Wick_small.mp4')\nvideo_writer = None\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nvideo_writer = cv2.VideoWriter('/kaggle/working/data/John_Wick_small_out1.mp4', fourcc, video_reader.fps,(video_reader.width, video_reader.height))\n\nfor frame in mmcv.track_iter_progress(video_reader):\n  result = inference_detector(model, frame)\n  frame = model.show_result(frame, result, score_thr=0.4)\n\n  video_writer.write(frame)\n\nif video_writer:\n        video_writer.release()","metadata":{"id":"MsM3nWsBKIBe","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### segmentation 시각화 함수 직접 작성 후 단일 이미지와 Video inference 수행.","metadata":{"id":"kZexB8Dvr1kW"}},{"cell_type":"code","source":"# 0부터 순차적으로 클래스 매핑된 label 적용.\nlabels_to_names_seq = {0:'person',1:'bicycle',2:'car',3:'motorbike',4:'aeroplane',5:'bus',6:'train',7:'truck',8:'boat',9:'traffic light',10:'fire hydrant',\n                        11:'stop sign',12:'parking meter',13:'bench',14:'bird',15:'cat',16:'dog',17:'horse',18:'sheep',19:'cow',20:'elephant',\n                        21:'bear',22:'zebra',23:'giraffe',24:'backpack',25:'umbrella',26:'handbag',27:'tie',28:'suitcase',29:'frisbee',30:'skis',\n                        31:'snowboard',32:'sports ball',33:'kite',34:'baseball bat',35:'baseball glove',36:'skateboard',37:'surfboard',38:'tennis racket',39:'bottle',40:'wine glass',\n                        41:'cup',42:'fork',43:'knife',44:'spoon',45:'bowl',46:'banana',47:'apple',48:'sandwich',49:'orange',50:'broccoli',\n                        51:'carrot',52:'hot dog',53:'pizza',54:'donut',55:'cake',56:'chair',57:'sofa',58:'pottedplant',59:'bed',60:'diningtable',\n                        61:'toilet',62:'tvmonitor',63:'laptop',64:'mouse',65:'remote',66:'keyboard',67:'cell phone',68:'microwave',69:'oven',70:'toaster',\n                        71:'sink',72:'refrigerator',73:'book',74:'clock',75:'vase',76:'scissors',77:'teddy bear',78:'hair drier',79:'toothbrush' }\n\ncolors = list(\n    [[0, 255, 0],\n     [0, 0, 255],\n     [255, 0, 0],\n     [0, 255, 255],\n     [255, 255, 0],\n     [255, 0, 255],\n     [80, 70, 180],\n     [250, 80, 190],\n     [245, 145, 50],\n     [70, 150, 250]] )","metadata":{"id":"ohYU8ottv5Zu","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#mask threshold값은 mask_thr_binary=0.5 로 지정되어 있음.\nprint(model.cfg.pretty_text)","metadata":{"id":"VlaaDH6E2Uel","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# model과 원본 이미지 array, filtering할 기준 class confidence score를 인자로 가지는 inference 시각화용 함수 생성.\n# 이미 inference 시 mask boolean값이 들어오므로 mask_threshold 값을 필요하지 않음.\ndef get_detected_img(model, img_array,  score_threshold=0.3, is_print=True):\n  # 인자로 들어온 image_array를 복사.\n  draw_img = img_array.copy()\n  bbox_color=(0, 255, 0)\n  text_color=(0, 0, 255)\n\n  # model과 image array를 입력 인자로 inference detection 수행하고 결과를 results로 받음.\n  # results는 80개의 2차원 array(shape=(오브젝트갯수, 5))를 가지는 list.\n  results = inference_detector(model, img_array)\n  bbox_results = results[0]\n  seg_results = results[1]\n\n  # 80개의 array원소를 가지는 results 리스트를 loop를 돌면서 개별 2차원 array들을 추출하고 이를 기반으로 이미지 시각화\n  # results 리스트의 위치 index가 바로 COCO 매핑된 Class id. 여기서는 result_ind가 class id\n  # 개별 2차원 array에 오브젝트별 좌표와 class confidence score 값을 가짐.\n  for result_ind, bbox_result in enumerate(bbox_results):\n    # 개별 2차원 array의 row size가 0 이면 해당 Class id로 값이 없으므로 다음 loop로 진행.\n    if len(bbox_result) == 0:\n      continue\n\n    mask_array_list = seg_results[result_ind]\n\n    # 해당 클래스 별로 Detect된 여러개의 오브젝트 정보가 2차원 array에 담겨 있으며, 이 2차원 array를 row수만큼 iteration해서 개별 오브젝트의 좌표값 추출.\n    for i in range(len(bbox_result)):\n      # 좌상단, 우하단 좌표 추출.\n      if bbox_result[i, 4] > score_threshold:\n        left = int(bbox_result[i, 0])\n        top = int(bbox_result[i, 1])\n        right = int(bbox_result[i, 2])\n        bottom = int(bbox_result[i, 3])\n        caption = \"{}: {:.4f}\".format(labels_to_names_seq[result_ind], bbox_result[i, 4])\n        cv2.rectangle(draw_img, (left, top), (right, bottom), color=bbox_color, thickness=2)\n        cv2.putText(draw_img, caption, (int(left), int(top - 7)), cv2.FONT_HERSHEY_SIMPLEX, 0.37, text_color, 1)\n        # masking 시각화 적용. class_mask_array는 image 크기 shape의  True/False값을 가지는 2차원 array\n        class_mask_array = mask_array_list[i]\n        # 원본 image array에서 mask가 True인 영역만 별도 추출.\n        masked_roi = draw_img[class_mask_array]\n        #color를 임의 지정\n        color_index = np.random.randint(0, len(colors)-1)\n        # color를 class별로 지정\n        #color_index = result_ind % len(colors)\n        color = colors[color_index]\n        # apply_mask()함수를 적용시 수행 시간이 상대적으로 오래 걸림.\n        #draw_img = apply_mask(draw_img, class_mask_array, color, alpha=0.4)\n        # 원본 이미지의 masking 될 영역에 mask를 특정 투명 컬러로 적용\n        draw_img[class_mask_array] = ([0.3*color[0], 0.3*color[1], 0.3*color[2]] + 0.6 * masked_roi).astype(np.uint8)\n        if is_print:\n          print(caption)\n\n  return draw_img\n","metadata":{"id":"MyzT-CAiv5c7","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"img_arr = cv2.imread('/kaggle/working/mmdetection/demo/demo.jpg')\nmask_array = results[1][2][0]\nprint('image array shape:', img_arr.shape)\nprint('mask array shape:', mask_array.shape)\nprint('mask true array shape:', mask_array[mask_array].shape)\nprint('masked array shape:', img_arr[results[1][2][0]].shape)\n","metadata":{"id":"GiK0Z8sqGi7l","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nimg_arr = cv2.imread('/kaggle/working/mmdetection/demo/demo.jpg')\ndetected_img = get_detected_img(model, img_arr,  score_threshold=0.3, is_print=True)\n# detect 입력된 이미지는 bgr임. 이를 최종 출력시 rgb로 변환\ndetected_img = cv2.cvtColor(detected_img, cv2.COLOR_BGR2RGB)\n\nplt.figure(figsize=(12, 12))\nplt.imshow(detected_img)","metadata":{"id":"0TTcVFEu43pQ","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!mkdir /kaggle/working/data\n!wget -O ./data/beatles01.jpg https://raw.githubusercontent.com/chulminkw/DLCV/master/data/image/beatles01.jpg","metadata":{"id":"kh54JphBA_Fa","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nimg_arr = cv2.imread('/kaggle/working/data/beatles01.jpg')\ndetected_img = get_detected_img(model, img_arr,  score_threshold=0.3, is_print=True)\n# detect 입력된 이미지는 bgr임. 이를 최종 출력시 rgb로 변환\ndetected_img = cv2.cvtColor(detected_img, cv2.COLOR_BGR2RGB)\n\nplt.figure(figsize=(12, 12))\nplt.imshow(detected_img)","metadata":{"id":"pmm3ySEW430t","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import time\n\ndef do_detected_video(model, input_path, output_path, score_threshold, do_print=True):\n\n    cap = cv2.VideoCapture(input_path)\n\n    codec = cv2.VideoWriter_fourcc(*'XVID')\n\n    vid_size = (round(cap.get(cv2.CAP_PROP_FRAME_WIDTH)),round(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)))\n    vid_fps = cap.get(cv2.CAP_PROP_FPS)\n\n    vid_writer = cv2.VideoWriter(output_path, codec, vid_fps, vid_size)\n\n    frame_cnt = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n    print('총 Frame 갯수:', frame_cnt)\n    btime = time.time()\n    while True:\n        hasFrame, img_frame = cap.read()\n        if not hasFrame:\n            print('더 이상 처리할 frame이 없습니다.')\n            break\n        stime = time.time()\n        img_frame = get_detected_img(model, img_frame,  score_threshold=score_threshold,is_print=False)\n        if do_print:\n          print('frame별 detection 수행 시간:', round(time.time() - stime, 4))\n        vid_writer.write(img_frame)\n    # end of while loop\n\n    vid_writer.release()\n    cap.release()\n\n    print('최종 detection 완료 수행 시간:', round(time.time() - btime, 4))","metadata":{"id":"j7kiQhPu4394","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"do_detected_video(model, '/kaggle/working/data/John_Wick_small.mp4', '/kaggle/working/data/John_Wick_small_out2.mp4', score_threshold=0.4, do_print=True)","metadata":{"id":"mznJS9NKKr03","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"id":"ykNwhsR4Kr4h","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"id":"RWzbe1ar1Plo","trusted":true},"outputs":[],"execution_count":null}]}
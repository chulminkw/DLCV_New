{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNq4DMPFHHgl1lkwv2U9eiz"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"c7XRP9bwSfJu"},"source":["### MMDetection 설치\n","* 강의 영상에는 pip install mmcv-full로 mmcv를 설치(약 10분 정도의 시간이 소요)\n","* 실습코드는 pip install mmcv-full -f https://download.openmmlab.com/mmcv/dist/cu116/torch1.13/index.html 로 변경(설치에 12초 정도 걸림. 2022.09).\n","*  2023년 4월 6일 기준으로 mmdetection이 3.0으로 upgrade됨. 실습 코드는 mmdetection 2.x 기준으므로 mmdetection 2.x 소스코드 설치 필요.\n","* 2024년 9월 colab의 numpy version이 1.24로 upgrade되면서 일부 코드가 동작오류. numpy 1.23 으로 downgrade 적용\n","* 2025년 1월 17일 Colab의 python 버전이 3.10에서 3.11로 버전업 되면서 pytorch 2.0, torchvision 0.15로 변경. mmcv도 !pip install mmcv-full -f https://download.openmmlab.com/mmcv/dist/cu118/torch2.0.0/index.html 로 변경."]},{"cell_type":"markdown","source":["#### pytorch, torchvision 다운그레이드"],"metadata":{"id":"BSjXKstzYkSJ"}},{"cell_type":"code","source":["#코랩의 pytorch 버전이 2.x 로 upgrade\n","import torch\n","print(torch.__version__)"],"metadata":{"id":"DKzS19hadG4_","executionInfo":{"status":"ok","timestamp":1671537019026,"user_tz":-540,"elapsed":2406,"user":{"displayName":"권철민","userId":"03917677622451543916"}},"outputId":"0155c2e7-add1-4804-dcfb-c244667d1ef5","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["1.13.0+cu116\n"]}]},{"cell_type":"code","metadata":{"id":"xJkO1dPo1-2b"},"source":["!pip install torch==2.0.0 torchvision==0.15.1  --index-url https://download.pytorch.org/whl/cu118"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### mmcv-full 및 mmdetection 설치"],"metadata":{"id":"-0QBU3E7YpWf"}},{"cell_type":"code","source":["# mmcv를 위해서 mmcv-full을 먼저 설치해야 함. https://mmcv.readthedocs.io/en/latest/get_started/installation.html 설치 과정 참조.\n","!pip install mmcv-full -f https://download.openmmlab.com/mmcv/dist/cu118/torch2.0.0/index.html"],"metadata":{"id":"eUGm5N_lcc_W"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 2023년 4월 6일 기준으로 mmdetection이 3.0으로 upgrade됨. 실습 코드는 mmdetection 2.x 기준임.\n","# mmdetection 2.x branch의 소스코드 기반으로 mmdetection 설치 필요.\n","!git clone --branch 2.x https://github.com/open-mmlab/mmdetection.git\n","!cd mmdetection; python setup.py install"],"metadata":{"id":"cThVMqmfcezJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KbAnpNg9Q8dG","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1671537144839,"user_tz":-540,"elapsed":6316,"user":{"displayName":"권철민","userId":"03917677622451543916"}},"outputId":"a7b4ed5a-c75d-48a5-d09f-ddf6ab511e27"},"source":["# 아래를 수행하기 전에 kernel을 restart 해야 함.\n","from mmdet.apis import init_detector, inference_detector\n","import mmcv"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.8/dist-packages/mmcv/__init__.py:20: UserWarning: On January 1, 2023, MMCV will release v2.0.0, in which it will remove components related to the training process and add a data transformation module. In addition, it will rename the package names mmcv to mmcv-lite and mmcv-full to mmcv. See https://github.com/open-mmlab/mmcv/blob/master/docs/en/compatibility.md for more details.\n","  warnings.warn(\n"]}]},{"cell_type":"code","source":["### 반드시 numpy downgrade를 mmcv 설치 후에 실행할것.\n","!pip install numpy==1.23"],"metadata":{"id":"3Fn7TFl0Yw9k"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tjsrX8SFSnK7"},"source":["### PASCAL VOC 2007 데이터 세트 다운로드"]},{"cell_type":"code","metadata":{"id":"rKHMaXju2EOE","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1671537186369,"user_tz":-540,"elapsed":25015,"user":{"displayName":"권철민","userId":"03917677622451543916"}},"outputId":"3cb8f9c1-d1f7-48a7-b89e-9a8b09c3fbdb"},"source":["!wget http://pjreddie.com/media/files/VOCtrainval_06-Nov-2007.tar\n","!tar -xvf VOCtrainval_06-Nov-2007.tar > /dev/null 2>&1"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["--2022-12-20 11:52:41--  http://pjreddie.com/media/files/VOCtrainval_06-Nov-2007.tar\n","Resolving pjreddie.com (pjreddie.com)... 128.208.4.108\n","Connecting to pjreddie.com (pjreddie.com)|128.208.4.108|:80... connected.\n","HTTP request sent, awaiting response... 301 Moved Permanently\n","Location: https://pjreddie.com/media/files/VOCtrainval_06-Nov-2007.tar [following]\n","--2022-12-20 11:52:42--  https://pjreddie.com/media/files/VOCtrainval_06-Nov-2007.tar\n","Connecting to pjreddie.com (pjreddie.com)|128.208.4.108|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 460032000 (439M) [application/octet-stream]\n","Saving to: ‘VOCtrainval_06-Nov-2007.tar’\n","\n","VOCtrainval_06-Nov- 100%[===================>] 438.72M  20.8MB/s    in 22s     \n","\n","2022-12-20 11:53:04 (19.9 MB/s) - ‘VOCtrainval_06-Nov-2007.tar’ saved [460032000/460032000]\n","\n"]}]},{"cell_type":"markdown","metadata":{"id":"-rvSmtd4SyIW"},"source":["### MMDetection은 Mask RCNN을 학습하기 위해서는 COCO 포맷을 가장 선호\n","* CocoDataset으로 지정해야만, evaluation 시 mask evaluation 정보 제공.(2021년 6월 기준) https://mmdetection.readthedocs.io/en/latest/tutorials/customize_dataset.html\n","* Pascal voc 포맷을 Coco 포맷으로 변환할 수 있는 유틸리티를 활용하여 데이터 변환\n","https://github.com/ISSResearch/Dataset-Converters\n","* Dataset converter 패키지가 opencv를 3.4로 downgrade함에 유의"]},{"cell_type":"code","metadata":{"id":"KJg7Mj7rYIlG","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1671537260152,"user_tz":-540,"elapsed":406,"user":{"displayName":"권철민","userId":"03917677622451543916"}},"outputId":"b27b8151-e6f4-4e5c-cb5b-c6bc9a15d287"},"source":["import cv2\n","print(cv2.__version__)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["4.6.0\n"]}]},{"cell_type":"code","metadata":{"id":"4KQXbBV7wBjO","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1671537262836,"user_tz":-540,"elapsed":1096,"user":{"displayName":"권철민","userId":"03917677622451543916"}},"outputId":"8d903a2f-97f5-4d66-dd37-596987d97d41"},"source":["!git clone https://github.com/ISSResearch/Dataset-Converters.git\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'Dataset-Converters'...\n","remote: Enumerating objects: 54, done.\u001b[K\n","remote: Counting objects: 100% (15/15), done.\u001b[K\n","remote: Compressing objects: 100% (13/13), done.\u001b[K\n","remote: Total 54 (delta 3), reused 10 (delta 2), pack-reused 39\u001b[K\n","Unpacking objects: 100% (54/54), done.\n"]}]},{"cell_type":"code","metadata":{"id":"rk8lq3PNjsig","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1671537277925,"user_tz":-540,"elapsed":13858,"user":{"displayName":"권철민","userId":"03917677622451543916"}},"outputId":"ad5bbe53-b0e8-447d-fbe9-6d84995b7cd7"},"source":["!cd Dataset-Converters; pip install -r requirements.txt"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting opencv-python<4.0,>=3.2\n","  Downloading opencv_python-3.4.18.65-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (58.4 MB)\n","\u001b[K     |████████████████████████████████| 58.4 MB 1.2 MB/s \n","\u001b[?25hRequirement already satisfied: future in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 2)) (0.16.0)\n","Collecting dicttoxml\n","  Downloading dicttoxml-1.7.15-py3-none-any.whl (24 kB)\n","Collecting xmltodict\n","  Downloading xmltodict-0.13.0-py2.py3-none-any.whl (10.0 kB)\n","Collecting cityscapesscripts\n","  Downloading cityscapesScripts-2.2.1-py3-none-any.whl (473 kB)\n","\u001b[K     |████████████████████████████████| 473 kB 70.5 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.8/dist-packages (from opencv-python<4.0,>=3.2->-r requirements.txt (line 1)) (1.21.6)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.8/dist-packages (from cityscapesscripts->-r requirements.txt (line 5)) (7.1.2)\n","Collecting pyquaternion\n","  Downloading pyquaternion-0.9.9-py3-none-any.whl (14 kB)\n","Requirement already satisfied: appdirs in /usr/local/lib/python3.8/dist-packages (from cityscapesscripts->-r requirements.txt (line 5)) (1.4.4)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from cityscapesscripts->-r requirements.txt (line 5)) (4.64.1)\n","Collecting typing\n","  Downloading typing-3.7.4.3.tar.gz (78 kB)\n","\u001b[K     |████████████████████████████████| 78 kB 7.8 MB/s \n","\u001b[?25hCollecting coloredlogs\n","  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n","\u001b[K     |████████████████████████████████| 46 kB 4.5 MB/s \n","\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.8/dist-packages (from cityscapesscripts->-r requirements.txt (line 5)) (3.2.2)\n","Collecting humanfriendly>=9.1\n","  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n","\u001b[K     |████████████████████████████████| 86 kB 7.0 MB/s \n","\u001b[?25hRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->cityscapesscripts->-r requirements.txt (line 5)) (1.4.4)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->cityscapesscripts->-r requirements.txt (line 5)) (3.0.9)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib->cityscapesscripts->-r requirements.txt (line 5)) (0.11.0)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->cityscapesscripts->-r requirements.txt (line 5)) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.1->matplotlib->cityscapesscripts->-r requirements.txt (line 5)) (1.15.0)\n","Building wheels for collected packages: typing\n","  Building wheel for typing (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for typing: filename=typing-3.7.4.3-py3-none-any.whl size=26323 sha256=759ee137e324f7ad2ce0f7892e7ce14c3766385b5fc0d409dd3890812a2c5a27\n","  Stored in directory: /root/.cache/pip/wheels/5e/5d/01/3083e091b57809dad979ea543def62d9d878950e3e74f0c930\n","Successfully built typing\n","Installing collected packages: humanfriendly, typing, pyquaternion, coloredlogs, xmltodict, opencv-python, dicttoxml, cityscapesscripts\n","  Attempting uninstall: opencv-python\n","    Found existing installation: opencv-python 4.6.0.66\n","    Uninstalling opencv-python-4.6.0.66:\n","      Successfully uninstalled opencv-python-4.6.0.66\n","Successfully installed cityscapesscripts-2.2.1 coloredlogs-15.0.1 dicttoxml-1.7.15 humanfriendly-10.0 opencv-python-3.4.18.65 pyquaternion-0.9.9 typing-3.7.4.3 xmltodict-0.13.0\n"]}]},{"cell_type":"code","metadata":{"id":"2aDFbNkjWQmh"},"source":["!mkdir /content/coco_output\n","!cd Dataset-Converters;python convert.py --input-folder /content/VOCdevkit/VOC2007 --output-folder /content/coco_output \\\n","                  --input-format VOCSEGM --output-format COCO --copy"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"h7xdA8F_ZT3E","colab":{"base_uri":"https://localhost:8080/","height":315},"executionInfo":{"status":"ok","timestamp":1671537292233,"user_tz":-540,"elapsed":6713,"user":{"displayName":"권철민","userId":"03917677622451543916"}},"outputId":"800c58e3-9ba5-4034-be35-c0de578fbf2f"},"source":["!pip install opencv-python==4.1.2.30"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting opencv-python==4.1.2.30\n","  Downloading opencv_python-4.1.2.30-cp38-cp38-manylinux1_x86_64.whl (28.3 MB)\n","\u001b[K     |████████████████████████████████| 28.3 MB 1.2 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.8/dist-packages (from opencv-python==4.1.2.30) (1.21.6)\n","Installing collected packages: opencv-python\n","  Attempting uninstall: opencv-python\n","    Found existing installation: opencv-python 3.4.18.65\n","    Uninstalling opencv-python-3.4.18.65:\n","      Successfully uninstalled opencv-python-3.4.18.65\n","Successfully installed opencv-python-4.1.2.30\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["cv2"]}}},"metadata":{}}]},{"cell_type":"markdown","metadata":{"id":"Wh0LZFGwbB_5"},"source":["### 생성된 Coco Annotation json 파일 살펴 보기"]},{"cell_type":"code","metadata":{"id":"TROJ8YQuaOTp","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1671537300124,"user_tz":-540,"elapsed":7896,"user":{"displayName":"권철민","userId":"03917677622451543916"}},"outputId":"63d4ecc0-7818-44ec-b927-b4f90fde8115"},"source":["!sudo apt-get install jq"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","The following package was automatically installed and is no longer required:\n","  libnvidia-common-460\n","Use 'sudo apt autoremove' to remove it.\n","The following additional packages will be installed:\n","  libjq1 libonig4\n","The following NEW packages will be installed:\n","  jq libjq1 libonig4\n","0 upgraded, 3 newly installed, 0 to remove and 20 not upgraded.\n","Need to get 276 kB of archives.\n","After this operation, 930 kB of additional disk space will be used.\n","Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libonig4 amd64 6.7.0-1 [119 kB]\n","Get:2 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libjq1 amd64 1.5+dfsg-2 [111 kB]\n","Get:3 http://archive.ubuntu.com/ubuntu bionic/universe amd64 jq amd64 1.5+dfsg-2 [45.6 kB]\n","Fetched 276 kB in 0s (3,259 kB/s)\n","debconf: unable to initialize frontend: Dialog\n","debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 3.)\n","debconf: falling back to frontend: Readline\n","debconf: unable to initialize frontend: Readline\n","debconf: (This frontend requires a controlling tty.)\n","debconf: falling back to frontend: Teletype\n","dpkg-preconfigure: unable to re-open stdin: \n","Selecting previously unselected package libonig4:amd64.\n","(Reading database ... 124016 files and directories currently installed.)\n","Preparing to unpack .../libonig4_6.7.0-1_amd64.deb ...\n","Unpacking libonig4:amd64 (6.7.0-1) ...\n","Selecting previously unselected package libjq1:amd64.\n","Preparing to unpack .../libjq1_1.5+dfsg-2_amd64.deb ...\n","Unpacking libjq1:amd64 (1.5+dfsg-2) ...\n","Selecting previously unselected package jq.\n","Preparing to unpack .../jq_1.5+dfsg-2_amd64.deb ...\n","Unpacking jq (1.5+dfsg-2) ...\n","Setting up libonig4:amd64 (6.7.0-1) ...\n","Setting up libjq1:amd64 (1.5+dfsg-2) ...\n","Setting up jq (1.5+dfsg-2) ...\n","Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n","Processing triggers for libc-bin (2.27-3ubuntu1.6) ...\n"]}]},{"cell_type":"code","metadata":{"id":"uX7X1MoSaOXM"},"source":["!jq . /content/coco_output/annotations/train.json > output.json"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OhzkICHXaOaa","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1671537300724,"user_tz":-540,"elapsed":9,"user":{"displayName":"권철민","userId":"03917677622451543916"}},"outputId":"ad18bdb4-972a-4963-bccf-36722af89fb5"},"source":["!head -200 output.json"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{\n","  \"images\": [\n","    {\n","      \"file_name\": \"000032.jpg\",\n","      \"height\": 281,\n","      \"width\": 500,\n","      \"id\": 1\n","    },\n","    {\n","      \"file_name\": \"000033.jpg\",\n","      \"height\": 366,\n","      \"width\": 500,\n","      \"id\": 2\n","    },\n","    {\n","      \"file_name\": \"000042.jpg\",\n","      \"height\": 335,\n","      \"width\": 500,\n","      \"id\": 3\n","    },\n","    {\n","      \"file_name\": \"000061.jpg\",\n","      \"height\": 333,\n","      \"width\": 500,\n","      \"id\": 4\n","    },\n","    {\n","      \"file_name\": \"000129.jpg\",\n","      \"height\": 500,\n","      \"width\": 334,\n","      \"id\": 5\n","    },\n","    {\n","      \"file_name\": \"000187.jpg\",\n","      \"height\": 375,\n","      \"width\": 500,\n","      \"id\": 6\n","    },\n","    {\n","      \"file_name\": \"000250.jpg\",\n","      \"height\": 375,\n","      \"width\": 500,\n","      \"id\": 7\n","    },\n","    {\n","      \"file_name\": \"000256.jpg\",\n","      \"height\": 343,\n","      \"width\": 500,\n","      \"id\": 8\n","    },\n","    {\n","      \"file_name\": \"000528.jpg\",\n","      \"height\": 375,\n","      \"width\": 500,\n","      \"id\": 9\n","    },\n","    {\n","      \"file_name\": \"000549.jpg\",\n","      \"height\": 500,\n","      \"width\": 375,\n","      \"id\": 10\n","    },\n","    {\n","      \"file_name\": \"000559.jpg\",\n","      \"height\": 370,\n","      \"width\": 500,\n","      \"id\": 11\n","    },\n","    {\n","      \"file_name\": \"000648.jpg\",\n","      \"height\": 333,\n","      \"width\": 500,\n","      \"id\": 12\n","    },\n","    {\n","      \"file_name\": \"000733.jpg\",\n","      \"height\": 435,\n","      \"width\": 450,\n","      \"id\": 13\n","    },\n","    {\n","      \"file_name\": \"000768.jpg\",\n","      \"height\": 375,\n","      \"width\": 500,\n","      \"id\": 14\n","    },\n","    {\n","      \"file_name\": \"000793.jpg\",\n","      \"height\": 375,\n","      \"width\": 500,\n","      \"id\": 15\n","    },\n","    {\n","      \"file_name\": \"000804.jpg\",\n","      \"height\": 500,\n","      \"width\": 333,\n","      \"id\": 16\n","    },\n","    {\n","      \"file_name\": \"000822.jpg\",\n","      \"height\": 374,\n","      \"width\": 500,\n","      \"id\": 17\n","    },\n","    {\n","      \"file_name\": \"000830.jpg\",\n","      \"height\": 375,\n","      \"width\": 500,\n","      \"id\": 18\n","    },\n","    {\n","      \"file_name\": \"000904.jpg\",\n","      \"height\": 375,\n","      \"width\": 500,\n","      \"id\": 19\n","    },\n","    {\n","      \"file_name\": \"000999.jpg\",\n","      \"height\": 500,\n","      \"width\": 375,\n","      \"id\": 20\n","    },\n","    {\n","      \"file_name\": \"001073.jpg\",\n","      \"height\": 375,\n","      \"width\": 500,\n","      \"id\": 21\n","    },\n","    {\n","      \"file_name\": \"001239.jpg\",\n","      \"height\": 375,\n","      \"width\": 500,\n","      \"id\": 22\n","    },\n","    {\n","      \"file_name\": \"001299.jpg\",\n","      \"height\": 500,\n","      \"width\": 429,\n","      \"id\": 23\n","    },\n","    {\n","      \"file_name\": \"001408.jpg\",\n","      \"height\": 500,\n","      \"width\": 333,\n","      \"id\": 24\n","    },\n","    {\n","      \"file_name\": \"001420.jpg\",\n","      \"height\": 332,\n","      \"width\": 500,\n","      \"id\": 25\n","    },\n","    {\n","      \"file_name\": \"001457.jpg\",\n","      \"height\": 375,\n","      \"width\": 500,\n","      \"id\": 26\n","    },\n","    {\n","      \"file_name\": \"001526.jpg\",\n","      \"height\": 298,\n","      \"width\": 500,\n","      \"id\": 27\n","    },\n","    {\n","      \"file_name\": \"001586.jpg\",\n","      \"height\": 398,\n","      \"width\": 480,\n","      \"id\": 28\n","    },\n","    {\n","      \"file_name\": \"001594.jpg\",\n","      \"height\": 333,\n","      \"width\": 500,\n","      \"id\": 29\n","    },\n","    {\n","      \"file_name\": \"001630.jpg\",\n","      \"height\": 500,\n","      \"width\": 375,\n","      \"id\": 30\n","    },\n","    {\n","      \"file_name\": \"001717.jpg\",\n","      \"height\": 404,\n","      \"width\": 500,\n","      \"id\": 31\n","    },\n","    {\n","      \"file_name\": \"001733.jpg\",\n","      \"height\": 500,\n","      \"width\": 500,\n","      \"id\": 32\n","    },\n","    {\n","      \"file_name\": \"001761.jpg\",\n","      \"height\": 333,\n","      \"width\": 500,\n","      \"id\": 33\n","    },\n"]}]},{"cell_type":"code","metadata":{"id":"Jg1nUKl6aOdC","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1671537300724,"user_tz":-540,"elapsed":5,"user":{"displayName":"권철민","userId":"03917677622451543916"}},"outputId":"dcb0b3fd-3236-4255-f888-386cd08e4dc4"},"source":["!tail -200 output.json"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["          31,\n","          304,\n","          34,\n","          305,\n","          35,\n","          305,\n","          38,\n","          308,\n","          41,\n","          307,\n","          42,\n","          307,\n","          43,\n","          306,\n","          44,\n","          307,\n","          45,\n","          307,\n","          46,\n","          306,\n","          47,\n","          306,\n","          48,\n","          308,\n","          50,\n","          309,\n","          49,\n","          312,\n","          49,\n","          313,\n","          50,\n","          313,\n","          49,\n","          314,\n","          48,\n","          314,\n","          46,\n","          315,\n","          45,\n","          315,\n","          43,\n","          316,\n","          42,\n","          316,\n","          40,\n","          317,\n","          39,\n","          317,\n","          37,\n","          318,\n","          36,\n","          318,\n","          34,\n","          319,\n","          33,\n","          319,\n","          31,\n","          320,\n","          30,\n","          320,\n","          20,\n","          321,\n","          19,\n","          321,\n","          17,\n","          322,\n","          16,\n","          322,\n","          11,\n","          321,\n","          10,\n","          321,\n","          7,\n","          322,\n","          6,\n","          322,\n","          1,\n","          303,\n","          1,\n","          302,\n","          0\n","        ]\n","      ],\n","      \"area\": 2888,\n","      \"iscrowd\": 0,\n","      \"image_id\": 209,\n","      \"bbox\": [\n","        242,\n","        0,\n","        81,\n","        51\n","      ],\n","      \"category_id\": 7,\n","      \"id\": 633,\n","      \"ignore\": 0\n","    }\n","  ],\n","  \"categories\": [\n","    {\n","      \"supercategory\": \"none\",\n","      \"id\": 1,\n","      \"name\": \"aeroplane\"\n","    },\n","    {\n","      \"supercategory\": \"none\",\n","      \"id\": 2,\n","      \"name\": \"bicycle\"\n","    },\n","    {\n","      \"supercategory\": \"none\",\n","      \"id\": 3,\n","      \"name\": \"bird\"\n","    },\n","    {\n","      \"supercategory\": \"none\",\n","      \"id\": 4,\n","      \"name\": \"boat\"\n","    },\n","    {\n","      \"supercategory\": \"none\",\n","      \"id\": 5,\n","      \"name\": \"bottle\"\n","    },\n","    {\n","      \"supercategory\": \"none\",\n","      \"id\": 6,\n","      \"name\": \"bus\"\n","    },\n","    {\n","      \"supercategory\": \"none\",\n","      \"id\": 7,\n","      \"name\": \"car\"\n","    },\n","    {\n","      \"supercategory\": \"none\",\n","      \"id\": 8,\n","      \"name\": \"cat\"\n","    },\n","    {\n","      \"supercategory\": \"none\",\n","      \"id\": 9,\n","      \"name\": \"chair\"\n","    },\n","    {\n","      \"supercategory\": \"none\",\n","      \"id\": 10,\n","      \"name\": \"cow\"\n","    },\n","    {\n","      \"supercategory\": \"none\",\n","      \"id\": 11,\n","      \"name\": \"diningtable\"\n","    },\n","    {\n","      \"supercategory\": \"none\",\n","      \"id\": 12,\n","      \"name\": \"dog\"\n","    },\n","    {\n","      \"supercategory\": \"none\",\n","      \"id\": 13,\n","      \"name\": \"horse\"\n","    },\n","    {\n","      \"supercategory\": \"none\",\n","      \"id\": 14,\n","      \"name\": \"motorbike\"\n","    },\n","    {\n","      \"supercategory\": \"none\",\n","      \"id\": 15,\n","      \"name\": \"person\"\n","    },\n","    {\n","      \"supercategory\": \"none\",\n","      \"id\": 16,\n","      \"name\": \"pottedplant\"\n","    },\n","    {\n","      \"supercategory\": \"none\",\n","      \"id\": 17,\n","      \"name\": \"sheep\"\n","    },\n","    {\n","      \"supercategory\": \"none\",\n","      \"id\": 18,\n","      \"name\": \"sofa\"\n","    },\n","    {\n","      \"supercategory\": \"none\",\n","      \"id\": 19,\n","      \"name\": \"train\"\n","    },\n","    {\n","      \"supercategory\": \"none\",\n","      \"id\": 20,\n","      \"name\": \"tvmonitor\"\n","    }\n","  ]\n","}\n"]}]},{"cell_type":"code","metadata":{"id":"_zTNFwsSakv6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1671537301194,"user_tz":-540,"elapsed":472,"user":{"displayName":"권철민","userId":"03917677622451543916"}},"outputId":"3b713a71-8550-481e-d3f6-b943f2b8db43"},"source":["!grep -n 'annotations' output.json"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["1259:  \"annotations\": [\n"]}]},{"cell_type":"code","metadata":{"id":"8hp4Ufz7apHW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1671537301194,"user_tz":-540,"elapsed":6,"user":{"displayName":"권철민","userId":"03917677622451543916"}},"outputId":"72aeecda-b52e-4690-c74e-1188a326da1c"},"source":["!head -1600 output.json | tail -400"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["      \"id\": 200\n","    },\n","    {\n","      \"file_name\": \"009654.jpg\",\n","      \"height\": 500,\n","      \"width\": 333,\n","      \"id\": 201\n","    },\n","    {\n","      \"file_name\": \"009684.jpg\",\n","      \"height\": 375,\n","      \"width\": 500,\n","      \"id\": 202\n","    },\n","    {\n","      \"file_name\": \"009691.jpg\",\n","      \"height\": 333,\n","      \"width\": 500,\n","      \"id\": 203\n","    },\n","    {\n","      \"file_name\": \"009709.jpg\",\n","      \"height\": 500,\n","      \"width\": 375,\n","      \"id\": 204\n","    },\n","    {\n","      \"file_name\": \"009756.jpg\",\n","      \"height\": 375,\n","      \"width\": 500,\n","      \"id\": 205\n","    },\n","    {\n","      \"file_name\": \"009807.jpg\",\n","      \"height\": 333,\n","      \"width\": 500,\n","      \"id\": 206\n","    },\n","    {\n","      \"file_name\": \"009832.jpg\",\n","      \"height\": 375,\n","      \"width\": 500,\n","      \"id\": 207\n","    },\n","    {\n","      \"file_name\": \"009911.jpg\",\n","      \"height\": 400,\n","      \"width\": 500,\n","      \"id\": 208\n","    },\n","    {\n","      \"file_name\": \"009938.jpg\",\n","      \"height\": 375,\n","      \"width\": 500,\n","      \"id\": 209\n","    }\n","  ],\n","  \"type\": \"instances\",\n","  \"annotations\": [\n","    {\n","      \"segmentation\": [\n","        [\n","          234,\n","          106,\n","          232,\n","          108,\n","          230,\n","          108,\n","          228,\n","          110,\n","          227,\n","          110,\n","          226,\n","          111,\n","          216,\n","          111,\n","          215,\n","          110,\n","          214,\n","          111,\n","          213,\n","          111,\n","          212,\n","          112,\n","          212,\n","          113,\n","          213,\n","          114,\n","          213,\n","          115,\n","          212,\n","          116,\n","          210,\n","          116,\n","          204,\n","          122,\n","          204,\n","          123,\n","          202,\n","          125,\n","          202,\n","          127,\n","          200,\n","          129,\n","          200,\n","          131,\n","          199,\n","          132,\n","          189,\n","          132,\n","          188,\n","          131,\n","          187,\n","          131,\n","          186,\n","          130,\n","          170,\n","          130,\n","          169,\n","          129,\n","          162,\n","          129,\n","          161,\n","          128,\n","          150,\n","          128,\n","          149,\n","          127,\n","          106,\n","          127,\n","          105,\n","          128,\n","          106,\n","          129,\n","          109,\n","          129,\n","          111,\n","          131,\n","          147,\n","          131,\n","          148,\n","          132,\n","          159,\n","          132,\n","          160,\n","          133,\n","          162,\n","          133,\n","          163,\n","          134,\n","          183,\n","          134,\n","          184,\n","          135,\n","          194,\n","          135,\n","          196,\n","          137,\n","          198,\n","          135,\n","          199,\n","          136,\n","          199,\n","          153,\n","          200,\n","          154,\n","          200,\n","          156,\n","          201,\n","          157,\n","          201,\n","          158,\n","          205,\n","          162,\n","          207,\n","          162,\n","          208,\n","          163,\n","          208,\n","          164,\n","          212,\n","          168,\n","          212,\n","          173,\n","          211,\n","          174,\n","          211,\n","          175,\n","          212,\n","          176,\n","          212,\n","          177,\n","          214,\n","          179,\n","          214,\n","          180,\n","          216,\n","          182,\n","          220,\n","          182,\n","          222,\n","          180,\n","          224,\n","          182,\n","          225,\n","          181,\n","          225,\n","          179,\n","          228,\n","          176,\n","          228,\n","          175,\n","          227,\n","          174,\n","          228,\n","          173,\n","          225,\n","          170,\n","          225,\n","          166,\n","          224,\n","          165,\n","          225,\n","          164,\n","          228,\n","          164,\n","          229,\n","          163,\n","          231,\n","          163,\n","          235,\n","          159,\n","          236,\n","          159,\n","          237,\n","          158,\n","          237,\n","          157,\n","          238,\n","          156,\n","          239,\n","          156,\n","          240,\n","          155,\n","          240,\n","          153,\n","          245,\n","          148,\n","          245,\n","          146,\n","          246,\n","          145,\n","          245,\n","          144,\n","          245,\n","          142,\n","          246,\n","          141,\n","          246,\n","          139,\n","          245,\n","          138,\n","          245,\n","          137,\n","          246,\n","          136,\n","          247,\n","          137,\n","          249,\n","          137,\n","          250,\n","          138,\n","          249,\n","          139,\n","          249,\n","          140,\n","          250,\n","          141,\n","          254,\n","          141,\n","          255,\n","          140,\n","          257,\n","          140,\n","          258,\n","          139,\n","          258,\n","          138,\n","          259,\n","          137,\n","          262,\n","          137,\n","          263,\n","          138,\n","          263,\n","          146,\n","          262,\n","          147,\n","          261,\n","          147,\n","          260,\n","          146,\n","          259,\n","          147,\n","          259,\n","          154,\n","          260,\n","          155,\n","          261,\n","          154,\n","          261,\n","          148,\n","          262,\n","          147,\n","          263,\n","          148,\n","          263,\n","          150,\n","          264,\n","          151,\n","          265,\n","          150,\n","          266,\n","          151,\n","          266,\n","          155,\n","          267,\n","          156,\n","          268,\n","          155,\n","          268,\n","          147,\n","          267,\n","          146,\n","          266,\n","          147,\n","          265,\n","          146,\n","          265,\n","          140,\n","          266,\n","          139,\n","          266,\n","          138,\n","          267,\n","          137,\n","          277,\n","          137,\n","          278,\n","          136,\n","          285,\n","          136,\n","          286,\n","          135,\n","          324,\n","          135,\n","          327,\n","          132,\n","          334,\n","          132,\n","          335,\n","          131,\n","          344,\n","          131,\n","          345,\n","          130,\n","          360,\n","          130,\n","          361,\n","          129,\n","          373,\n","          129,\n","          374,\n","          128,\n","          373,\n","          127,\n","          361,\n","          127,\n","          360,\n","          128,\n","          345,\n","          128,\n","          344,\n","          129,\n","          335,\n","          129,\n","          334,\n","          130,\n","          326,\n","          130,\n","          325,\n","          131,\n","          294,\n","          131,\n","          293,\n","          132,\n","          284,\n","          132,\n","          283,\n","          133,\n"]}]},{"cell_type":"markdown","metadata":{"id":"dBldbcCSbUj3"},"source":["### Pretrained 모델 다운로드 및 Config, Dataset설정."]},{"cell_type":"code","metadata":{"id":"0bEeppPQG_sr"},"source":["# pretrained weight 모델을 다운로드 받기 위해서 mmdetection/checkpoints 디렉토리를 만듬.\n","!cd mmdetection; mkdir checkpoints"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KwcBRmjdQDLQ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1671537305950,"user_tz":-540,"elapsed":4758,"user":{"displayName":"권철민","userId":"03917677622451543916"}},"outputId":"4c748a59-fb87-4e5e-b152-62c56676259b"},"source":["!wget -O /content/mmdetection/checkpoints/mask_rcnn_r101_fpn_1x_coco_20200204-1efe0ed5.pth http://download.openmmlab.com/mmdetection/v2.0/mask_rcnn/mask_rcnn_r101_fpn_1x_coco/mask_rcnn_r101_fpn_1x_coco_20200204-1efe0ed5.pth"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["--2022-12-20 11:55:01--  http://download.openmmlab.com/mmdetection/v2.0/mask_rcnn/mask_rcnn_r101_fpn_1x_coco/mask_rcnn_r101_fpn_1x_coco_20200204-1efe0ed5.pth\n","Resolving download.openmmlab.com (download.openmmlab.com)... 47.246.48.207, 47.246.48.204, 47.246.48.206, ...\n","Connecting to download.openmmlab.com (download.openmmlab.com)|47.246.48.207|:80... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 254089599 (242M) [application/octet-stream]\n","Saving to: ‘/content/mmdetection/checkpoints/mask_rcnn_r101_fpn_1x_coco_20200204-1efe0ed5.pth’\n","\n","/content/mmdetectio 100%[===================>] 242.32M  53.3MB/s    in 4.4s    \n","\n","2022-12-20 11:55:06 (54.7 MB/s) - ‘/content/mmdetection/checkpoints/mask_rcnn_r101_fpn_1x_coco_20200204-1efe0ed5.pth’ saved [254089599/254089599]\n","\n"]}]},{"cell_type":"code","metadata":{"id":"Ifz2eS2lBqMJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1671537306906,"user_tz":-540,"elapsed":958,"user":{"displayName":"권철민","userId":"03917677622451543916"}},"outputId":"fa914a85-4713-4426-970f-881d937587f7"},"source":["!ls -lia /content/mmdetection/checkpoints"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["total 248144\n","2113086 drwxr-xr-x  2 root root      4096 Dec 20 11:55 .\n","2097333 drwxr-xr-x 19 root root      4096 Dec 20 11:55 ..\n","2113087 -rw-r--r--  1 root root 254089599 Nov  2  2021 mask_rcnn_r101_fpn_1x_coco_20200204-1efe0ed5.pth\n"]}]},{"cell_type":"code","metadata":{"id":"3uTqmO5EQDOC"},"source":["# config 파일을 설정하고, 다운로드 받은 pretrained 모델을 checkpoint로 설정.\n","config_file = '/content/mmdetection/configs/mask_rcnn/mask_rcnn_r101_fpn_1x_coco.py'\n","checkpoint_file = '/content/mmdetection/checkpoints/mask_rcnn_r101_fpn_1x_coco_20200204-1efe0ed5.pth'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WKPZrx6HQDQe"},"source":["from mmdet.datasets.builder import DATASETS\n","from mmdet.datasets.coco import CocoDataset\n","\n","@DATASETS.register_module(force=True)\n","class VOCDataset(CocoDataset):\n","  CLASSES = ('aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car',\n","               'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse',\n","               'motorbike', 'person', 'pottedplant', 'sheep', 'sofa', 'train',\n","               'tvmonitor')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VrPweafhQDTd","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1671537306907,"user_tz":-540,"elapsed":5,"user":{"displayName":"권철민","userId":"03917677622451543916"}},"outputId":"bf55f48a-c30b-4a4f-cf5f-3fe10f3d246a"},"source":["from mmcv import Config\n","\n","cfg = Config.fromfile(config_file)\n","print(cfg.pretty_text)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["model = dict(\n","    type='MaskRCNN',\n","    backbone=dict(\n","        type='ResNet',\n","        depth=101,\n","        num_stages=4,\n","        out_indices=(0, 1, 2, 3),\n","        frozen_stages=1,\n","        norm_cfg=dict(type='BN', requires_grad=True),\n","        norm_eval=True,\n","        style='pytorch',\n","        init_cfg=dict(type='Pretrained',\n","                      checkpoint='torchvision://resnet101')),\n","    neck=dict(\n","        type='FPN',\n","        in_channels=[256, 512, 1024, 2048],\n","        out_channels=256,\n","        num_outs=5),\n","    rpn_head=dict(\n","        type='RPNHead',\n","        in_channels=256,\n","        feat_channels=256,\n","        anchor_generator=dict(\n","            type='AnchorGenerator',\n","            scales=[8],\n","            ratios=[0.5, 1.0, 2.0],\n","            strides=[4, 8, 16, 32, 64]),\n","        bbox_coder=dict(\n","            type='DeltaXYWHBBoxCoder',\n","            target_means=[0.0, 0.0, 0.0, 0.0],\n","            target_stds=[1.0, 1.0, 1.0, 1.0]),\n","        loss_cls=dict(\n","            type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0),\n","        loss_bbox=dict(type='L1Loss', loss_weight=1.0)),\n","    roi_head=dict(\n","        type='StandardRoIHead',\n","        bbox_roi_extractor=dict(\n","            type='SingleRoIExtractor',\n","            roi_layer=dict(type='RoIAlign', output_size=7, sampling_ratio=0),\n","            out_channels=256,\n","            featmap_strides=[4, 8, 16, 32]),\n","        bbox_head=dict(\n","            type='Shared2FCBBoxHead',\n","            in_channels=256,\n","            fc_out_channels=1024,\n","            roi_feat_size=7,\n","            num_classes=80,\n","            bbox_coder=dict(\n","                type='DeltaXYWHBBoxCoder',\n","                target_means=[0.0, 0.0, 0.0, 0.0],\n","                target_stds=[0.1, 0.1, 0.2, 0.2]),\n","            reg_class_agnostic=False,\n","            loss_cls=dict(\n","                type='CrossEntropyLoss', use_sigmoid=False, loss_weight=1.0),\n","            loss_bbox=dict(type='L1Loss', loss_weight=1.0)),\n","        mask_roi_extractor=dict(\n","            type='SingleRoIExtractor',\n","            roi_layer=dict(type='RoIAlign', output_size=14, sampling_ratio=0),\n","            out_channels=256,\n","            featmap_strides=[4, 8, 16, 32]),\n","        mask_head=dict(\n","            type='FCNMaskHead',\n","            num_convs=4,\n","            in_channels=256,\n","            conv_out_channels=256,\n","            num_classes=80,\n","            loss_mask=dict(\n","                type='CrossEntropyLoss', use_mask=True, loss_weight=1.0))),\n","    train_cfg=dict(\n","        rpn=dict(\n","            assigner=dict(\n","                type='MaxIoUAssigner',\n","                pos_iou_thr=0.7,\n","                neg_iou_thr=0.3,\n","                min_pos_iou=0.3,\n","                match_low_quality=True,\n","                ignore_iof_thr=-1),\n","            sampler=dict(\n","                type='RandomSampler',\n","                num=256,\n","                pos_fraction=0.5,\n","                neg_pos_ub=-1,\n","                add_gt_as_proposals=False),\n","            allowed_border=-1,\n","            pos_weight=-1,\n","            debug=False),\n","        rpn_proposal=dict(\n","            nms_pre=2000,\n","            max_per_img=1000,\n","            nms=dict(type='nms', iou_threshold=0.7),\n","            min_bbox_size=0),\n","        rcnn=dict(\n","            assigner=dict(\n","                type='MaxIoUAssigner',\n","                pos_iou_thr=0.5,\n","                neg_iou_thr=0.5,\n","                min_pos_iou=0.5,\n","                match_low_quality=True,\n","                ignore_iof_thr=-1),\n","            sampler=dict(\n","                type='RandomSampler',\n","                num=512,\n","                pos_fraction=0.25,\n","                neg_pos_ub=-1,\n","                add_gt_as_proposals=True),\n","            mask_size=28,\n","            pos_weight=-1,\n","            debug=False)),\n","    test_cfg=dict(\n","        rpn=dict(\n","            nms_pre=1000,\n","            max_per_img=1000,\n","            nms=dict(type='nms', iou_threshold=0.7),\n","            min_bbox_size=0),\n","        rcnn=dict(\n","            score_thr=0.05,\n","            nms=dict(type='nms', iou_threshold=0.5),\n","            max_per_img=100,\n","            mask_thr_binary=0.5)))\n","dataset_type = 'CocoDataset'\n","data_root = 'data/coco/'\n","img_norm_cfg = dict(\n","    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\n","train_pipeline = [\n","    dict(type='LoadImageFromFile'),\n","    dict(type='LoadAnnotations', with_bbox=True, with_mask=True),\n","    dict(type='Resize', img_scale=(1333, 800), keep_ratio=True),\n","    dict(type='RandomFlip', flip_ratio=0.5),\n","    dict(\n","        type='Normalize',\n","        mean=[123.675, 116.28, 103.53],\n","        std=[58.395, 57.12, 57.375],\n","        to_rgb=True),\n","    dict(type='Pad', size_divisor=32),\n","    dict(type='DefaultFormatBundle'),\n","    dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels', 'gt_masks'])\n","]\n","test_pipeline = [\n","    dict(type='LoadImageFromFile'),\n","    dict(\n","        type='MultiScaleFlipAug',\n","        img_scale=(1333, 800),\n","        flip=False,\n","        transforms=[\n","            dict(type='Resize', keep_ratio=True),\n","            dict(type='RandomFlip'),\n","            dict(\n","                type='Normalize',\n","                mean=[123.675, 116.28, 103.53],\n","                std=[58.395, 57.12, 57.375],\n","                to_rgb=True),\n","            dict(type='Pad', size_divisor=32),\n","            dict(type='ImageToTensor', keys=['img']),\n","            dict(type='Collect', keys=['img'])\n","        ])\n","]\n","data = dict(\n","    samples_per_gpu=2,\n","    workers_per_gpu=2,\n","    train=dict(\n","        type='CocoDataset',\n","        ann_file='data/coco/annotations/instances_train2017.json',\n","        img_prefix='data/coco/train2017/',\n","        pipeline=[\n","            dict(type='LoadImageFromFile'),\n","            dict(type='LoadAnnotations', with_bbox=True, with_mask=True),\n","            dict(type='Resize', img_scale=(1333, 800), keep_ratio=True),\n","            dict(type='RandomFlip', flip_ratio=0.5),\n","            dict(\n","                type='Normalize',\n","                mean=[123.675, 116.28, 103.53],\n","                std=[58.395, 57.12, 57.375],\n","                to_rgb=True),\n","            dict(type='Pad', size_divisor=32),\n","            dict(type='DefaultFormatBundle'),\n","            dict(\n","                type='Collect',\n","                keys=['img', 'gt_bboxes', 'gt_labels', 'gt_masks'])\n","        ]),\n","    val=dict(\n","        type='CocoDataset',\n","        ann_file='data/coco/annotations/instances_val2017.json',\n","        img_prefix='data/coco/val2017/',\n","        pipeline=[\n","            dict(type='LoadImageFromFile'),\n","            dict(\n","                type='MultiScaleFlipAug',\n","                img_scale=(1333, 800),\n","                flip=False,\n","                transforms=[\n","                    dict(type='Resize', keep_ratio=True),\n","                    dict(type='RandomFlip'),\n","                    dict(\n","                        type='Normalize',\n","                        mean=[123.675, 116.28, 103.53],\n","                        std=[58.395, 57.12, 57.375],\n","                        to_rgb=True),\n","                    dict(type='Pad', size_divisor=32),\n","                    dict(type='ImageToTensor', keys=['img']),\n","                    dict(type='Collect', keys=['img'])\n","                ])\n","        ]),\n","    test=dict(\n","        type='CocoDataset',\n","        ann_file='data/coco/annotations/instances_val2017.json',\n","        img_prefix='data/coco/val2017/',\n","        pipeline=[\n","            dict(type='LoadImageFromFile'),\n","            dict(\n","                type='MultiScaleFlipAug',\n","                img_scale=(1333, 800),\n","                flip=False,\n","                transforms=[\n","                    dict(type='Resize', keep_ratio=True),\n","                    dict(type='RandomFlip'),\n","                    dict(\n","                        type='Normalize',\n","                        mean=[123.675, 116.28, 103.53],\n","                        std=[58.395, 57.12, 57.375],\n","                        to_rgb=True),\n","                    dict(type='Pad', size_divisor=32),\n","                    dict(type='ImageToTensor', keys=['img']),\n","                    dict(type='Collect', keys=['img'])\n","                ])\n","        ]))\n","evaluation = dict(metric=['bbox', 'segm'])\n","optimizer = dict(type='SGD', lr=0.02, momentum=0.9, weight_decay=0.0001)\n","optimizer_config = dict(grad_clip=None)\n","lr_config = dict(\n","    policy='step',\n","    warmup='linear',\n","    warmup_iters=500,\n","    warmup_ratio=0.001,\n","    step=[8, 11])\n","runner = dict(type='EpochBasedRunner', max_epochs=12)\n","checkpoint_config = dict(interval=1)\n","log_config = dict(interval=50, hooks=[dict(type='TextLoggerHook')])\n","custom_hooks = [dict(type='NumClassCheckHook')]\n","dist_params = dict(backend='nccl')\n","log_level = 'INFO'\n","load_from = None\n","resume_from = None\n","workflow = [('train', 1)]\n","opencv_num_threads = 0\n","mp_start_method = 'fork'\n","auto_scale_lr = dict(enable=False, base_batch_size=16)\n","\n"]}]},{"cell_type":"code","metadata":{"id":"udEntY3Gj8VP"},"source":["from mmdet.apis import set_random_seed\n","\n","# dataset에 대한 환경 파라미터 수정.\n","cfg.dataset_type = 'VOCDataset'\n","cfg.data_root = '/content/coco_output/'\n","\n","# train, val, test dataset에 대한 type, data_root, ann_file, img_prefix 환경 파라미터 수정.\n","cfg.data.train.type = 'VOCDataset'\n","cfg.data.train.data_root = '/content/coco_output/'\n","cfg.data.train.ann_file = 'annotations/train.json'\n","cfg.data.train.img_prefix = 'train'\n","\n","cfg.data.val.type = 'VOCDataset'\n","cfg.data.val.data_root = '/content/coco_output/'\n","cfg.data.val.ann_file = 'annotations/val.json'\n","cfg.data.val.img_prefix = 'val'\n","\n","\n","# class의 갯수를 pascal voc로 설정.  수정.\n","cfg.model.roi_head.bbox_head.num_classes = 20\n","cfg.model.roi_head.mask_head.num_classes = 20\n","\n","# pretrained 모델\n","cfg.load_from = '/content/mmdetection/checkpoints/mask_rcnn_r101_fpn_1x_coco_20200204-1efe0ed5.pth'\n","\n","# 학습 weight 파일로 로그를 저장하기 위한 디렉토리 설정.\n","cfg.work_dir = './tutorial_exps'\n","\n","# 학습율 변경 환경 파라미터 설정.\n","cfg.optimizer.lr = 0.02 / 8\n","cfg.lr_config.warmup = None\n","cfg.log_config.interval = 10\n","\n","# CocoDataset의 경우 metric을 bbox로 설정해야 함.(mAP아님. bbox로 설정하면 mAP를 iou threshold를 0.5 ~ 0.95까지 변경하면서 측정)\n","cfg.evaluation.metric = ['bbox', 'segm']\n","cfg.evaluation.interval = 12\n","cfg.checkpoint_config.interval = 12\n","\n","# 두번 config를 로드하면 lr_config의 policy가 사라지는 오류로 인하여 설정.\n","cfg.lr_config.policy='step'\n","# Set seed thus the results are more reproducible\n","cfg.seed = 0\n","set_random_seed(0, deterministic=False)\n","cfg.gpu_ids = range(1)\n","\n","# ConfigDict' object has no attribute 'device 오류 발생시 반드시 설정 필요. https://github.com/open-mmlab/mmdetection/issues/7901\n","cfg.device='cuda'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KXbuUXRyj8Yt"},"source":["print(cfg.pretty_text)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wd2Bik8Fj8cD"},"source":["from mmdet.datasets import build_dataset\n","from mmdet.models import build_detector\n","from mmdet.apis import train_detector\n","\n","# train용 Dataset 생성.\n","datasets = [build_dataset(cfg.data.train)]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"u7YF5h7Hj8ft"},"source":["model = build_detector(cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\n","model.CLASSES = datasets[0].CLASSES\n","print(model.CLASSES)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VgxxeD9Jj8ia"},"source":["import os.path as osp\n","mmcv.mkdir_or_exist(osp.abspath(cfg.work_dir))\n","# epochs는 config의 runner 파라미터로 지정됨. 기본 12회\n","train_detector(model, datasets, cfg, distributed=False, validate=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IEm1hGrzdV0o"},"source":["### 학습된 모델을 이용하여 단일 이미지와 Video Inference 수행."]},{"cell_type":"code","metadata":{"id":"IS78SaUh5giE"},"source":["from mmdet.apis import show_result_pyplot\n","\n","checkpoint_file = '/content/tutorial_exps/epoch_12.pth'\n","\n","# checkpoint 저장된 model 파일을 이용하여 모델을 생성, 이때 Config는 위에서 update된 config 사용.\n","model_ckpt = init_detector(cfg, checkpoint_file, device='cuda:0')\n","# BGR Image 사용\n","img = cv2.imread('/content/VOCdevkit/VOC2007/JPEGImages/000007.jpg')\n","#model_ckpt.cfg = cfg\n","\n","result = inference_detector(model_ckpt, img)\n","show_result_pyplot(model_ckpt, img, result, score_thr=0.5)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-zQONtMS6bRn"},"source":["!mkdir /content/data\n","!wget -O ./data/beatles01.jpg https://raw.githubusercontent.com/chulminkw/DLCV/master/data/image/beatles01.jpg"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nEAdcLjA6bgt"},"source":["img = cv2.imread('/content/data/beatles01.jpg')\n","#model_ckpt.cfg = cfg\n","\n","result = inference_detector(model_ckpt, img)\n","show_result_pyplot(model_ckpt, img, result, score_thr=0.5)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jcX5PLi37cy2"},"source":["### Video Inference 수행\n","* get_detected_img()함수를 이용하여 Inference 수행."]},{"cell_type":"code","metadata":{"id":"4UKrr1Hfj8nx"},"source":["!wget -O /content/data/London_Street.mp4 https://github.com/chulminkw/DLCV/blob/master/data/video/London_Street.mp4?raw=true"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OuOty1BK7da2"},"source":["import numpy as np\n","\n","labels_to_names_seq =  {0:'aeroplane', 1:'bicycle', 2:'bird', 3:'boat', 4:'bottle', 5:'bus', 6:'car',\n","               7:'cat', 8:'chair', 9:'cow', 10:'diningtable', 11:'dog', 12:'horse',\n","               13:'motorbike', 14:'person', 15:'pottedplant', 16:'sheep', 17:'sofa', 18:'train',\n","               19:'tvmonitor'}\n","\n","colors = list(\n","    [[0, 255, 0],\n","     [0, 0, 255],\n","     [255, 0, 0],\n","     [0, 255, 255],\n","     [255, 255, 0],\n","     [255, 0, 255],\n","     [80, 70, 180],\n","     [250, 80, 190],\n","     [245, 145, 50],\n","     [70, 150, 250]] )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qgNcOvwm7QbQ"},"source":["# model과 원본 이미지 array, filtering할 기준 class confidence score를 인자로 가지는 inference 시각화용 함수 생성.\n","# 이미 inference 시 mask boolean값이 들어오므로 mask_threshold 값을 필요하지 않음.\n","def get_detected_img(model, img_array,  score_threshold=0.3, is_print=True):\n","  # 인자로 들어온 image_array를 복사.\n","  draw_img = img_array.copy()\n","  bbox_color=(0, 255, 0)\n","  text_color=(0, 0, 255)\n","\n","  # model과 image array를 입력 인자로 inference detection 수행하고 결과를 results로 받음.\n","  # results는 80개의 2차원 array(shape=(오브젝트갯수, 5))를 가지는 list.\n","  results = inference_detector(model, img_array)\n","  bbox_results = results[0]\n","  seg_results = results[1]\n","\n","  # 80개의 array원소를 가지는 results 리스트를 loop를 돌면서 개별 2차원 array들을 추출하고 이를 기반으로 이미지 시각화\n","  # results 리스트의 위치 index가 바로 COCO 매핑된 Class id. 여기서는 result_ind가 class id\n","  # 개별 2차원 array에 오브젝트별 좌표와 class confidence score 값을 가짐.\n","  for result_ind, bbox_result in enumerate(bbox_results):\n","    # 개별 2차원 array의 row size가 0 이면 해당 Class id로 값이 없으므로 다음 loop로 진행.\n","    if len(bbox_result) == 0:\n","      continue\n","\n","    mask_array_list = seg_results[result_ind]\n","\n","    # 해당 클래스 별로 Detect된 여러개의 오브젝트 정보가 2차원 array에 담겨 있으며, 이 2차원 array를 row수만큼 iteration해서 개별 오브젝트의 좌표값 추출.\n","    for i in range(len(bbox_result)):\n","      # 좌상단, 우하단 좌표 추출.\n","      if bbox_result[i, 4] > score_threshold:\n","        left = int(bbox_result[i, 0])\n","        top = int(bbox_result[i, 1])\n","        right = int(bbox_result[i, 2])\n","        bottom = int(bbox_result[i, 3])\n","        caption = \"{}: {:.4f}\".format(labels_to_names_seq[result_ind], bbox_result[i, 4])\n","        cv2.rectangle(draw_img, (left, top), (right, bottom), color=bbox_color, thickness=2)\n","        cv2.putText(draw_img, caption, (int(left), int(top - 7)), cv2.FONT_HERSHEY_SIMPLEX, 0.37, text_color, 1)\n","        # masking 시각화 적용. class_mask_array는 image 크기 shape의  True/False값을 가지는 2차원 array\n","        class_mask_array = mask_array_list[i]\n","        # 원본 image array에서 mask가 True인 영역만 별도 추출.\n","        masked_roi = draw_img[class_mask_array]\n","        #color를 임의 지정\n","        #color_index = np.random.randint(0, len(colors)-1)\n","        # color를 class별로 지정\n","        color_index = result_ind % len(colors)\n","        color = colors[color_index]\n","        # apply_mask()함수를 적용시 수행 시간이 상대적으로 오래 걸림.\n","        #draw_img = apply_mask(draw_img, class_mask_array, color, alpha=0.4)\n","        # 원본 이미지의 masking 될 영역에 mask를 특정 투명 컬러로 적용\n","        draw_img[class_mask_array] = ([0.3*color[0], 0.3*color[1], 0.3*color[2]] + 0.6 * masked_roi).astype(np.uint8)\n","        if is_print:\n","          print(caption)\n","\n","  return draw_img\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rOnTeurI78Ny"},"source":["import time\n","\n","def do_detected_video(model, input_path, output_path, score_threshold, do_print=True):\n","\n","    cap = cv2.VideoCapture(input_path)\n","\n","    codec = cv2.VideoWriter_fourcc(*'XVID')\n","\n","    vid_size = (round(cap.get(cv2.CAP_PROP_FRAME_WIDTH)),round(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)))\n","    vid_fps = cap.get(cv2.CAP_PROP_FPS)\n","\n","    vid_writer = cv2.VideoWriter(output_path, codec, vid_fps, vid_size)\n","\n","    frame_cnt = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n","    print('총 Frame 갯수:', frame_cnt)\n","    btime = time.time()\n","    while True:\n","        hasFrame, img_frame = cap.read()\n","        if not hasFrame:\n","            print('더 이상 처리할 frame이 없습니다.')\n","            break\n","        stime = time.time()\n","        img_frame = get_detected_img(model, img_frame,  score_threshold=score_threshold,is_print=False)\n","        if do_print:\n","          print('frame별 detection 수행 시간:', round(time.time() - stime, 4))\n","        vid_writer.write(img_frame)\n","    # end of while loop\n","\n","    vid_writer.release()\n","    cap.release()\n","\n","    print('최종 detection 완료 수행 시간:', round(time.time() - btime, 4))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"g3vr964M8EGl"},"source":["do_detected_video(model_ckpt, '/content/data/London_Street.mp4', '/content/data/London_Street_out01.mp4', score_threshold=0.4, do_print=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UPJra-WN8IyA"},"source":[],"execution_count":null,"outputs":[]}]}
{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[]},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"Zr4A7LiVkNnv"},"source":["### MMDetection 설치\n","* 강의 영상에는 pip install mmcv-full로 mmcv를 설치(약 10분 정도의 시간이 소요)\n","* 실습코드는 pip install mmcv-full -f https://download.openmmlab.com/mmcv/dist/cu116/torch1.13/index.html 로 변경(설치에 12초 정도 걸림. 2022.09).\n","*  2023년 4월 6일 기준으로 mmdetection이 3.0으로 upgrade됨. 실습 코드는 mmdetection 2.x 기준으므로 mmdetection 2.x 소스코드 설치 필요.\n","* 2024년 9월 colab의 numpy version이 1.24로 upgrade되면서 일부 코드가 동작오류. numpy 1.23 으로 downgrade 적용\n","* 2025년 1월 17일 Colab의 python 버전이 3.10에서 3.11로 버전업 되면서 pytorch 2.0, torchvision 0.15로 변경. mmcv도 !pip install mmcv-full -f https://download.openmmlab.com/mmcv/dist/cu118/torch2.0.0/index.html 로 변경."]},{"cell_type":"markdown","source":["#### pytorch, torchvision 다운그레이드"],"metadata":{"id":"lM9HV1UUVPxa"}},{"cell_type":"code","source":["#코랩의 pytorch 버전이 2.x 로 upgrade\n","import torch\n","print(torch.__version__)"],"metadata":{"id":"rPlHbX7uQUa4"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wT0CFXNXObby"},"source":["!pip install torch==2.0.0 torchvision==0.15.1  --index-url https://download.pytorch.org/whl/cu118"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### mmcv-full 및 mmdetection 설치"],"metadata":{"id":"Py8pDKkWVWDe"}},{"cell_type":"code","metadata":{"trusted":true,"id":"zdMSR63rkK0C"},"source":["# mmcv를 위해서 mmcv-full을 먼저 설치해야 함. https://mmcv.readthedocs.io/en/latest/get_started/installation.html 설치 과정 참조.\n","!pip install mmcv-full -f https://download.openmmlab.com/mmcv/dist/cu118/torch2.0.0/index.html"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 2023년 4월 6일 기준으로 mmdetection이 3.0으로 upgrade됨. 실습 코드는 mmdetection 2.x 기준임.\n","# mmdetection 2.x branch의 소스코드 기반으로 mmdetection 설치 필요.\n","!git clone --branch 2.x https://github.com/open-mmlab/mmdetection.git\n","!cd mmdetection; python setup.py install"],"metadata":{"id":"u16-5M2ca9Po"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-05-27T06:12:23.626916Z","iopub.execute_input":"2021-05-27T06:12:23.627446Z","iopub.status.idle":"2021-05-27T06:12:36.122974Z","shell.execute_reply.started":"2021-05-27T06:12:23.627359Z","shell.execute_reply":"2021-05-27T06:12:36.122014Z"},"trusted":true,"id":"Chqi6PKikK0J"},"source":["# 아래를 수행하기 전에 kernel을 restart 해야 함.\n","from mmdet.apis import init_detector, inference_detector\n","import mmcv"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### 반드시 numpy downgrade를 mmcv 설치 후에 실행할것.\n","!pip install numpy==1.23"],"metadata":{"id":"bKQuo_dUVdXL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RbJUt3KCkK0M"},"source":["### Oxford Pet Dataset 다운로드\n","image와 annotations을 압축파일로 각각 download 수행."]},{"cell_type":"code","metadata":{"trusted":true,"id":"qnNSafQgkK0O"},"source":["!wget https://www.robots.ox.ac.uk/~vgg/data/pets/data/images.tar.gz\n","!wget https://www.robots.ox.ac.uk/~vgg/data/pets/data/annotations.tar.gz"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"NYIdnysnkK0P"},"source":["# /content/data 디렉토리를 만들고 해당 디렉토리에 다운로드 받은 압축 파일 풀기.\n","!mkdir /content/data\n","!tar -xvf images.tar.gz -C /content/data\n","!tar -xvf annotations.tar.gz -C /content/data"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"468iC6rKkK0Q"},"source":["### 이미지 디렉토리와 annotation 파일 살펴 보기"]},{"cell_type":"code","metadata":{"trusted":true,"id":"BgniG93SkK0R"},"source":["!ls -lia ./data/images/Abyss*.jpg"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"ErNR2x1TkK0S"},"source":["!ls -lia ./data/images"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"WxDRuchokK0T"},"source":["!cat ./data/annotations/xmls/Abyssinian_1.xml"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"5kZxptFEkK0T"},"source":["import glob\n","import xml.etree.ElementTree as ET\n","\n","# annotation xml 파일 파싱해서 bbox정보 추출\n","def get_bboxes_from_xml_test(xml_file):\n","  tree = ET.parse(xml_file)\n","  root = tree.getroot()\n","  bbox_names = []\n","  bboxes = []\n","  # 파일내에 있는 모든 object Element를 찾음.\n","  for obj in root.findall('object'):\n","\n","    bbox_name = obj.find('name').text\n","    xmlbox = obj.find('bndbox')\n","    x1 = int(xmlbox.find('xmin').text)\n","    y1 = int(xmlbox.find('ymin').text)\n","    x2 = int(xmlbox.find('xmax').text)\n","    y2 = int(xmlbox.find('ymax').text)\n","\n","    bbox_names.append(bbox_name)\n","    bboxes.append([x1, y1, x2, y2])\n","\n","  return bbox_names, bboxes\n","\n","get_bboxes_from_xml_test('./data/annotations/xmls/Abyssinian_1.xml')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"hfRC8pIQkK0W"},"source":["!ls -lia ./data/annotations/xmls/Abys*.xml"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"z0S-4oXfkK0W"},"source":["### train, val image/annotation 메타 파일 보기\n","* train과 valid 데이터로 나뉠 image와 annotation의 파일명을 가지는 메타 파일\n","* train과 valid용 meta 파일을 별도로 만듬."]},{"cell_type":"code","metadata":{"trusted":true,"id":"9RAnxyE-kK0X"},"source":["!cd ./data/annotations; cat trainval.txt"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-05-27T06:12:44.300178Z","iopub.execute_input":"2021-05-27T06:12:44.300610Z","iopub.status.idle":"2021-05-27T06:12:44.330506Z","shell.execute_reply.started":"2021-05-27T06:12:44.300571Z","shell.execute_reply":"2021-05-27T06:12:44.329741Z"},"trusted":true,"id":"gsfIawuekK0Y"},"source":["import pandas as pd\n","\n","pet_df = pd.read_csv('./data/annotations/trainval.txt', sep=' ', header=None, names=['img_name', 'class_id', 'etc1', 'etc2'])\n","pet_df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"DS5_q-EGkK0b"},"source":["pet_df['class_id'].value_counts()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-05-27T06:13:28.728005Z","iopub.execute_input":"2021-05-27T06:13:28.728352Z","iopub.status.idle":"2021-05-27T06:13:28.742705Z","shell.execute_reply.started":"2021-05-27T06:13:28.728307Z","shell.execute_reply":"2021-05-27T06:13:28.741632Z"},"trusted":true,"id":"c2kgnuVhkK0c"},"source":["pet_df['class_name'] = pet_df['img_name'].apply(lambda x:x[:x.rfind('_')])\n","pet_df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"chZFqtmZkK0d"},"source":["from sklearn.model_selection import train_test_split\n","\n","train_df, val_df = train_test_split(pet_df, test_size=0.1, stratify=pet_df['class_id'], random_state=2021)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"up3CeUfbkK0e"},"source":["print(train_df['class_id'].value_counts(), val_df['class_id'].value_counts())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"Dtck1P1SkK0e"},"source":["train_df = train_df.sort_values(by='img_name')\n","val_df = val_df.sort_values(by='img_name')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"-kSL6xVKkK0f"},"source":["# ann_file로 주어지는 메타파일은 가급적이면 소스데이터의 가장 상단 디렉토리에 저장하는 것이 바람직.\n","train_df['img_name'].to_csv('./data/train.txt', sep=' ', header=False, index=False)\n","val_df['img_name'].to_csv('./data/val.txt', sep=' ', header=False, index=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_df.head()"],"metadata":{"id":"BPuPir0UyYLU"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"0F0enZeBkK0f"},"source":["pet_classes_list = pet_df['class_name'].unique().tolist()\n","print(pet_classes_list)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"fcIZeeQvkK0f"},"source":["!echo 'train list #####'; cat ./data/train.txt"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"OFtvaflTkK0g"},"source":["!echo ' valid list ###'; cat ./data/val.txt"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CgwtsjEakK0g"},"source":["### mmdetection의 중립 annotation 포맷 변환\n","* CLASSES 는 pet_df의 'class_name' 컬럼에 unique 데이터로 지정. class id는 tuple(list)형의 CLASSES의 index값에 따라 설정.\n","* ann_file로 입력되는 메타 파일을 읽어서 개별 image정보와 ann 정보를 dict로 생성하여 data_infos list에 입력\n","* 개별 XML 읽어서 ann 정보를 만드는 것은 get_bboxes_from_xml() 함수 이용.\n","* 디버깅용으로 CustomDataset을 만들어서 미리 테스트 하는 방법도 고려.\n","\n","![mmdetection_anno.png](data:image/png;base64,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)"]},{"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-05-27T06:15:31.218794Z","iopub.execute_input":"2021-05-27T06:15:31.219158Z","iopub.status.idle":"2021-05-27T06:15:31.231625Z","shell.execute_reply.started":"2021-05-27T06:15:31.219119Z","shell.execute_reply":"2021-05-27T06:15:31.230830Z"},"trusted":true,"id":"I7G465eMkK0j"},"source":["import xml.etree.ElementTree as ET\n","\n","# 1개의 annotation 파일에서 bbox 정보 추출. 여러개의 object가 있을 경우 이들 object의 name과 bbox 좌표들을 list로 반환.\n","def get_bboxes_from_xml(anno_dir, xml_file):\n","  anno_xml_file = osp.join(anno_dir, xml_file)\n","  tree = ET.parse(anno_xml_file)\n","  root = tree.getroot()\n","  bbox_names = []\n","  bboxes = []\n","\n","  # 파일내에 있는 모든 object Element를 찾음.\n","  for obj in root.findall('object'):\n","    #obj.find('name').text는 cat 이나 dog을 반환\n","    #bbox_name = obj.find('name').text\n","    # object의 클래스명은 파일명에서 추출.\n","    bbox_name = xml_file[:xml_file.rfind('_')]\n","\n","    xmlbox = obj.find('bndbox')\n","    x1 = int(xmlbox.find('xmin').text)\n","    y1 = int(xmlbox.find('ymin').text)\n","    x2 = int(xmlbox.find('xmax').text)\n","    y2 = int(xmlbox.find('ymax').text)\n","\n","    bboxes.append([x1, y1, x2, y2])\n","    bbox_names.append(bbox_name)\n","\n","  return bbox_names, bboxes"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-05-27T06:15:31.201850Z","iopub.execute_input":"2021-05-27T06:15:31.202205Z","iopub.status.idle":"2021-05-27T06:15:31.217153Z","shell.execute_reply.started":"2021-05-27T06:15:31.202172Z","shell.execute_reply":"2021-05-27T06:15:31.216213Z"},"trusted":true,"id":"Wh3h8h1ZkK0h"},"source":["import copy\n","import os.path as osp\n","\n","import mmcv\n","import numpy as np\n","import cv2\n","\n","from mmdet.datasets.builder import DATASETS\n","from mmdet.datasets.custom import CustomDataset\n","\n","import xml.etree.ElementTree as ET\n","\n","PET_CLASSES = pet_df['class_name'].unique().tolist()\n","\n","@DATASETS.register_module(force=True)\n","class PetDataset(CustomDataset):\n","  CLASSES = PET_CLASSES\n","\n","  # annotation에 대한 모든 파일명을 가지고 있는 텍스트 파일을 __init__(self, ann_file)로 입력 받고,\n","  # 이 self.ann_file이 load_annotations()의 인자로 입력\n","  def load_annotations(self, ann_file):\n","    cat2label = {k:i for i, k in enumerate(self.CLASSES)}\n","    image_list = mmcv.list_from_file(self.ann_file)\n","    # 포맷 중립 데이터를 담을 list 객체\n","    data_infos = []\n","\n","    for image_id in image_list:\n","      # self.img_prefix는 images 가 입력될 것임.\n","      filename = '{0:}/{1:}.jpg'.format(self.img_prefix, image_id)\n","      # 원본 이미지의 너비, 높이를 image를 직접 로드하여 구함.\n","      image = cv2.imread(filename)\n","      height, width = image.shape[:2]\n","      # 개별 image의 annotation 정보 저장용 Dict 생성. key값 filename에는 image의 파일명만 들어감(디렉토리는 제외)\n","      # 영상에는 data_info = {'filename': filename 으로 되어 있으나 filename은 image 파일명만 들어가는게 맞음.\n","      data_info = {'filename': str(image_id) + '.jpg',\n","                  'width': width, 'height': height}\n","      # 개별 annotation XML 파일이 있는 서브 디렉토리의 prefix 변환.\n","      label_prefix = self.img_prefix.replace('images', 'annotations')\n","\n","      # 개별 annotation XML 파일을 1개 line 씩 읽어서 list 로드. annotation XML파일이 xmls 밑에 있음에 유의\n","      anno_xml_file = osp.join(label_prefix, 'xmls/'+str(image_id)+'.xml')\n","      # 메타 파일에는 이름이 있으나 실제로는 존재하지 않는 XML이 있으므로 이는 제외.\n","      if not osp.exists(anno_xml_file):\n","          continue\n","\n","      # get_bboxes_from_xml() 를 이용하여 개별 XML 파일에 있는 이미지의 모든 bbox 정보를 list 객체로 생성.\n","      anno_dir = osp.join(label_prefix, 'xmls')\n","      bbox_names, bboxes = get_bboxes_from_xml(anno_dir, str(image_id)+'.xml')\n","      #print('#########:', bbox_names)\n","\n","      gt_bboxes = []\n","      gt_labels = []\n","      gt_bboxes_ignore = []\n","      gt_labels_ignore = []\n","\n","      # bbox별 Object들의 class name을 class id로 매핑. class id는 tuple(list)형의 CLASSES의 index값에 따라 설정\n","      for bbox_name, bbox in zip(bbox_names, bboxes):\n","        # 만약 bbox_name이 클래스명에 해당 되면, gt_bboxes와 gt_labels에 추가, 그렇지 않으면 gt_bboxes_ignore, gt_labels_ignore에 추가\n","        # bbox_name이 CLASSES중에 반드시 하나 있어야 함. 안 그러면 FILTERING 되므로 주의 할것.\n","        if bbox_name in cat2label:\n","            gt_bboxes.append(bbox)\n","            # gt_labels에는 class id를 입력\n","            gt_labels.append(cat2label[bbox_name])\n","        else:\n","            gt_bboxes_ignore.append(bbox)\n","            gt_labels_ignore.append(-1)\n","\n","      # 개별 image별 annotation 정보를 가지는 Dict 생성. 해당 Dict의 value값을 np.array형태로 bbox의 좌표와 label값으로 생성.\n","      data_anno = {\n","        'bboxes': np.array(gt_bboxes, dtype=np.float32).reshape(-1, 4),\n","        'labels': np.array(gt_labels, dtype=np.long),\n","        'bboxes_ignore': np.array(gt_bboxes_ignore, dtype=np.float32).reshape(-1, 4),\n","        'labels_ignore': np.array(gt_labels_ignore, dtype=np.long)\n","      }\n","\n","      # image에 대한 메타 정보를 가지는 data_info Dict에 'ann' key값으로 data_anno를 value로 저장.\n","      data_info.update(ann=data_anno)\n","      # 전체 annotation 파일들에 대한 정보를 가지는 data_infos에 data_info Dict를 추가\n","      data_infos.append(data_info)\n","      #print(data_info)\n","\n","    return data_infos\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["mmcv.list_from_file('/content/data/train.txt')"],"metadata":{"id":"ZB2dSujvvjiV"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"13JRMKBp1oLk"},"source":["''' 디버깅 용도로 CustomDataset을 흉내낸 클래스를 생성하여 다양한 테스트를 수행 가능 '''\n","\n","import os.path as osp\n","\n","PET_CLASSES = pet_df['class_name'].unique().tolist()\n","\n","# 디버깅 용도로 CustomDataset을 흉내낸 클래스 생성.\n","class PetDataset_imsi():\n","  CLASSES = PET_CLASSES\n","\n","  # 생성자 함수 생성.\n","  def __init__(self, data_root, ann_file, img_prefix):\n","      self.data_root = data_root\n","      self.ann_file = osp.join(data_root, ann_file)\n","      self.img_prefix = osp.join(data_root, img_prefix)\n","\n","      self.data_infos = self.load_annotations(self.ann_file)\n","\n","  # annotation에 대한 모든 파일명을 가지고 있는 텍스트 파일을 __init__(self, ann_file)로 입력 받고,\n","  # 이 self.ann_file이 load_annotations()의 인자로 입력\n","  def load_annotations(self, ann_file):\n","    cat2label = {k:i for i, k in enumerate(self.CLASSES)}\n","    image_list = mmcv.list_from_file(self.ann_file)\n","    # 포맷 중립 데이터를 담을 list 객체\n","    data_infos = []\n","\n","    for image_id in image_list:\n","      # self.img_prefix는 images 가 입력될 것임.\n","      filename = '{0:}/{1:}.jpg'.format(self.img_prefix, image_id)\n","      # 원본 이미지의 너비, 높이를 image를 직접 로드하여 구함.\n","      image = cv2.imread(filename)\n","      height, width = image.shape[:2]\n","      # 개별 image의 annotation 정보 저장용 Dict 생성. key값 filename에는 image의 파일명만 들어감(디렉토리는 제외)\n","      # 영상에는 data_info = {'filename': filename 으로 되어 있으나 filename은 image 파일명만 들어가는게 맞음.\n","      data_info = {'filename': str(image_id) + '.jpg',\n","                  'width': width, 'height': height}\n","      # 개별 annotation XML 파일이 있는 서브 디렉토리의 prefix 변환.\n","      label_prefix = self.img_prefix.replace('images', 'annotations')\n","\n","      # 개별 annotation XML 파일을 1개 line 씩 읽어서 list 로드. annotation XML파일이 xmls 밑에 있음에 유의\n","      anno_xml_file = osp.join(label_prefix, 'xmls/'+str(image_id)+'.xml')\n","      # 메타 파일에는 이름이 있으나 실제로는 존재하지 않는 XML이 있으므로 이는 제외.\n","      if not osp.exists(anno_xml_file):\n","          continue\n","\n","      # get_bboxes_from_xml() 를 이용하여 개별 XML 파일에 있는 이미지의 모든 bbox 정보를 list 객체로 생성.\n","      anno_dir = osp.join(label_prefix, 'xmls')\n","      bbox_names, bboxes = get_bboxes_from_xml(anno_dir, str(image_id)+'.xml')\n","      #print('#########:', bbox_names)\n","\n","      gt_bboxes = []\n","      gt_labels = []\n","      gt_bboxes_ignore = []\n","      gt_labels_ignore = []\n","\n","      # bbox별 Object들의 class name을 class id로 매핑. class id는 tuple(list)형의 CLASSES의 index값에 따라 설정\n","      for bbox_name, bbox in zip(bbox_names, bboxes):\n","        # 만약 bbox_name이 클래스명에 해당 되면, gt_bboxes와 gt_labels에 추가, 그렇지 않으면 gt_bboxes_ignore, gt_labels_ignore에 추가\n","        # bbox_name이 CLASSES중에 반드시 하나 있어야 함. 안 그러면 FILTERING 되므로 주의 할것.\n","        if bbox_name in cat2label:\n","            gt_bboxes.append(bbox)\n","            # gt_labels에는 class id를 입력\n","            gt_labels.append(cat2label[bbox_name])\n","        else:\n","            gt_bboxes_ignore.append(bbox)\n","            gt_labels_ignore.append(-1)\n","\n","      # 개별 image별 annotation 정보를 가지는 Dict 생성. 해당 Dict의 value값을 np.array형태로 bbox의 좌표와 label값으로 생성.\n","      data_anno = {\n","        'bboxes': np.array(gt_bboxes, dtype=np.float32).reshape(-1, 4),\n","        'labels': np.array(gt_labels, dtype=np.long),\n","        'bboxes_ignore': np.array(gt_bboxes_ignore, dtype=np.float32).reshape(-1, 4),\n","        'labels_ignore': np.array(gt_labels_ignore, dtype=np.long)\n","      }\n","\n","      # image에 대한 메타 정보를 가지는 data_info Dict에 'ann' key값으로 data_anno를 value로 저장.\n","      data_info.update(ann=data_anno)\n","      # 전체 annotation 파일들에 대한 정보를 가지는 data_infos에 data_info Dict를 추가\n","      data_infos.append(data_info)\n","      #print(data_info)\n","\n","    return data_infos\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-05-27T06:14:23.506508Z","iopub.execute_input":"2021-05-27T06:14:23.506850Z","iopub.status.idle":"2021-05-27T06:14:34.825408Z","shell.execute_reply.started":"2021-05-27T06:14:23.506817Z","shell.execute_reply":"2021-05-27T06:14:34.824471Z"},"trusted":true,"id":"deWhFR2LkK0j"},"source":["# 디버깅 용도로 생성한 클래스를 생성하고 data_infos를 10개만 추출하여 생성된 데이터 확인.\n","train_ds = PetDataset_imsi(data_root='/content/data', ann_file='train.txt', img_prefix='images')\n","print(train_ds.data_infos[:10])"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(train_ds.data_infos[:10])"],"metadata":{"id":"crit9vUFAl0G"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3XcTEPGikK0k"},"source":["### Config를 설정하고 COCO로 Pretrained 된 모델을 Download\n","* config파일은 faster rcnn resnet 50 backbone 사용.\n","* Oxford Pet 데이터는 학습에 시간에 소모 되므로 학습으로 생성된 모델을 Google Drive에 저장"]},{"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-05-27T06:25:37.151583Z","iopub.execute_input":"2021-05-27T06:25:37.151955Z","iopub.status.idle":"2021-05-27T06:25:37.156186Z","shell.execute_reply.started":"2021-05-27T06:25:37.151923Z","shell.execute_reply":"2021-05-27T06:25:37.155206Z"},"trusted":true,"id":"IB-Th_sXkK0l"},"source":["config_file = './mmdetection/configs/faster_rcnn/faster_rcnn_r50_fpn_1x_coco.py'\n","checkpoint_file = './mmdetection/checkpoints/faster_rcnn_r50_fpn_1x_coco_20200130-047c8118.pth'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-05-27T06:33:28.380407Z","iopub.execute_input":"2021-05-27T06:33:28.380757Z","iopub.status.idle":"2021-05-27T06:33:49.065382Z","shell.execute_reply.started":"2021-05-27T06:33:28.380726Z","shell.execute_reply":"2021-05-27T06:33:49.064292Z"},"trusted":true,"id":"vh8qg1i9kK0l"},"source":["!cd mmdetection; mkdir checkpoints\n","!wget -O ./mmdetection/checkpoints/faster_rcnn_r50_fpn_1x_coco_20200130-047c8118.pth http://download.openmmlab.com/mmdetection/v2.0/faster_rcnn/faster_rcnn_r50_fpn_1x_coco/faster_rcnn_r50_fpn_1x_coco_20200130-047c8118.pth"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-05-27T06:25:38.214427Z","iopub.execute_input":"2021-05-27T06:25:38.214765Z","iopub.status.idle":"2021-05-27T06:25:38.540441Z","shell.execute_reply.started":"2021-05-27T06:25:38.214734Z","shell.execute_reply":"2021-05-27T06:25:38.539557Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true,"id":"lDynqedDkK0l"},"source":["from mmcv import Config\n","\n","cfg = Config.fromfile(config_file)\n","print(cfg.pretty_text)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2PekQOUSqQZ3"},"source":["# Google Drive 접근을 위한 Mount 적용.\n","import os, sys\n","from google.colab import drive\n","\n","drive.mount('/content/gdrive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xc2DjYj1KAce"},"source":["# soft link로 Google Drive Directory 연결.\n","!ln -s /content/gdrive/My\\ Drive/ /mydrive\n","!ls /mydrive"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NAqx-sQ6qbYI"},"source":["# Google Drive 밑에 Directory 생성. 이미 생성 되어 있을 시 오류 발생.\n","!mkdir \"/mydrive/pet_work_dir\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AtvWjy_pLaG_"},"source":["!nvidia-smi"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-05-27T06:33:55.470309Z","iopub.execute_input":"2021-05-27T06:33:55.470692Z","iopub.status.idle":"2021-05-27T06:33:56.015738Z","shell.execute_reply.started":"2021-05-27T06:33:55.470656Z","shell.execute_reply":"2021-05-27T06:33:56.014821Z"},"trusted":true,"id":"yqNaYmsikK0m"},"source":["from mmdet.apis import set_random_seed\n","\n","# dataset에 대한 환경 파라미터 수정.\n","cfg.dataset_type = 'PetDataset'\n","cfg.data_root = '/content/data/'\n","\n","# train, val, test dataset에 대한 type, data_root, ann_file, img_prefix 환경 파라미터 수정.\n","cfg.data.train.type = 'PetDataset'\n","cfg.data.train.data_root = '/content/data/'\n","cfg.data.train.ann_file = 'train.txt'\n","cfg.data.train.img_prefix = 'images'\n","\n","cfg.data.val.type = 'PetDataset'\n","cfg.data.val.data_root = '/content/data/'\n","cfg.data.val.ann_file = 'val.txt'\n","cfg.data.val.img_prefix = 'images'\n","\n","# class의 갯수 수정.\n","cfg.model.roi_head.bbox_head.num_classes = 37\n","# pretrained 모델\n","cfg.load_from = 'checkpoints/faster_rcnn_r50_fpn_1x_coco_20200130-047c8118.pth'\n","\n","# 학습 weight 파일로 로그를 저장하기 위한 디렉토리로 구글 Drive 설정.\n","cfg.work_dir = '/mydrive/pet_work_dir'\n","\n","# 학습율 변경 환경 파라미터 설정.\n","cfg.optimizer.lr = 0.02 / 8\n","cfg.lr_config.warmup = None\n","cfg.log_config.interval = 5\n","\n","cfg.runner.max_epochs = 5\n","\n","# 평가 metric 설정.\n","cfg.evaluation.metric = 'mAP'\n","# 평가 metric 수행할 epoch interval 설정.\n","cfg.evaluation.interval = 5\n","# 학습 iteration시마다 모델을 저장할 epoch interval 설정.\n","cfg.checkpoint_config.interval = 5\n","\n","# 학습 시 Batch size 설정(단일 GPU 별 Batch size로 설정됨)\n","cfg.data.samples_per_gpu = 4\n","\n","# Set seed thus the results are more reproducible\n","cfg.seed = 0\n","set_random_seed(0, deterministic=False)\n","cfg.gpu_ids = range(1)\n","# 두번 config를 로드하면 lr_config의 policy가 사라지는 오류로 인하여 설정.\n","cfg.lr_config.policy='step'\n","\n","# ConfigDict' object has no attribute 'device 오류 발생시 반드시 설정 필요. https://github.com/open-mmlab/mmdetection/issues/7901\n","cfg.device='cuda'\n","\n","# We can initialize the logger for training and have a look\n","# at the final config used for training\n","print(f'Config:\\n{cfg.pretty_text}')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"o5aVbmLvlVOk"},"source":["### Train용 데이터를 생성하고 Oxford Dataset을 학습수행.\n","* build_dataset()로 train config 설정에 따른 Train용 dataset 생성.\n","* build_detector()로 train과 test config반영하여 model 생성.\n","* train_detector()로 model 학습.  "]},{"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-05-27T06:33:57.036213Z","iopub.execute_input":"2021-05-27T06:33:57.036592Z","iopub.status.idle":"2021-05-27T06:34:08.557918Z","shell.execute_reply.started":"2021-05-27T06:33:57.036557Z","shell.execute_reply":"2021-05-27T06:34:08.557074Z"},"trusted":true,"id":"c8eS_bH6kK0o"},"source":["from mmdet.datasets import build_dataset\n","from mmdet.models import build_detector\n","from mmdet.apis import train_detector\n","\n","# train용 Dataset 생성.\n","datasets = [build_dataset(cfg.data.train)]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OZIKIGNGdBhD"},"source":["datasets"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-05-27T06:34:08.559252Z","iopub.execute_input":"2021-05-27T06:34:08.559638Z"},"trusted":true,"id":"Oqdyf3aTkK0p"},"source":["%cd mmdetection\n","\n","model = build_detector(cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\n","model.CLASSES = datasets[0].CLASSES\n","\n","mmcv.mkdir_or_exist(osp.abspath(cfg.work_dir))\n","# epochs는 config의 runner 파라미터로 지정됨. 기본 12회\n","train_detector(model, datasets, cfg, distributed=False, validate=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hvXNhkhNrwF2"},"source":["|!ls -lia /content/data/images/english_cocker_spaniel*"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lr5U5cTQoXKa"},"source":[],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"t3KvDsjcs9qk"},"source":["### 학습된 model을 이용하여 inference 수행.\n","* 현재 memory에서 학습된 model 및 checkpoint 기록된 model을 loading하여 inference 수행.  "]},{"cell_type":"code","metadata":{"id":"HXKprlp4s-3b"},"source":["from mmdet.apis import inference_detector, init_detector, show_result_pyplot\n","\n","# BGR Image 사용\n","img = cv2.imread('/content/data/images/Abyssinian_88.jpg')\n","\n","model.cfg = cfg\n","\n","result = inference_detector(model, img)\n","show_result_pyplot(model, img, result, score_thr=0.3)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lZRVOaW8-0JH"},"source":["result"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dGpd5joetbw6"},"source":["# 아래는 오류를 발생시킵니다. 현재 Customized학습된 모델의 inference시 image file로 인자가 주어졌을때 inference 오류 발생.\n","img_path = '/content/data/images/Abyssinian_88.jpg'\n","\n","model.cfg = cfg\n","\n","result = inference_detector(model, img_path)\n","show_result_pyplot(model, img, result, score_thr=0.3)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5l6nVtPTO-cK"},"source":["### Checkpoint 저장된 model 파일을 로딩하고 이를 이용하여 Inference 수행."]},{"cell_type":"code","metadata":{"id":"oWB1nN1q8Vnp"},"source":["from mmdet.apis import show_result_pyplot\n","\n","checkpoint_file = '/mydrive/pet_work_dir/epoch_5.pth'\n","\n","# checkpoint 저장된 model 파일을 이용하여 모델을 생성, 이때 Config는 위에서 update된 config 사용.\n","model_ckpt = init_detector(cfg, checkpoint_file, device='cuda:0')\n","# BGR Image 사용\n","img = cv2.imread('/content/data/images/Abyssinian_88.jpg')\n","#model_ckpt.cfg = cfg\n","\n","result = inference_detector(model_ckpt, img)\n","show_result_pyplot(model_ckpt, img, result, score_thr=0.3)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9XYuhliUcic0"},"source":["### 여러개의 image들을 Inference 수행.\n","* inference_detector(model, imgs) 에서 인자 imgs는 단일 이미지일 경우 string/array, 여러개의 이미지일 경우 list(string/array)를 입력\n","* show_result_pyplot(model_ckpt, img, result, score_thr=0.3)는 여러개의 이미지를 한번에 나타내기 어려우므로 별도의 시각화 함수 get_detected_img()를 이용"]},{"cell_type":"code","metadata":{"id":"2l_DYC09jqYj"},"source":["val_df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MTGHOZ1ShLgM"},"source":["val_df['img_path'] = '/content/data/images/' + val_df['img_name'] + '.jpg'\n","val_df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AeJQlPKsj-0g"},"source":["val_df[val_df['img_path'].str.contains('Abyssinian')]['img_path'].values"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0UIvXnJCV0r9"},"source":["val_paths = val_df[val_df['img_path'].str.contains('Abyssinian')]['img_path'].values\n","val_imgs = [cv2.imread(x) for x in val_paths]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dI5kpbT0WD5p"},"source":["type(val_imgs), len(val_imgs), val_imgs[0].shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"g-sQKKHZR8Fs"},"source":["# 반환된 results는 개별 원소로 list를 가지는 list임.\n","results = inference_detector(model_ckpt, val_imgs)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jt3bbMm1Tt3o"},"source":["len(results), len(results[0]), results"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6kDWfFi4-T-Q"},"source":["PET_CLASSES = pet_df['class_name'].unique().tolist()\n","labels_to_names_seq = {i:k for i, k in enumerate(PET_CLASSES)}\n","\n","# model과 원본 이미지 array, filtering할 기준 class confidence score를 인자로 가지는 inference 시각화용 함수 생성.\n","def get_detected_img(model, img_array,  score_threshold=0.3, is_print=True):\n","  # 인자로 들어온 image_array를 복사.\n","  draw_img = img_array.copy()\n","  bbox_color=(0, 255, 0)\n","  text_color=(0, 0, 255)\n","\n","  # model과 image array를 입력 인자로 inference detection 수행하고 결과를 results로 받음.\n","  # results는 80개의 2차원 array(shape=(오브젝트갯수, 5))를 가지는 list.\n","  results = inference_detector(model, img_array)\n","\n","  # 80개의 array원소를 가지는 results 리스트를 loop를 돌면서 개별 2차원 array들을 추출하고 이를 기반으로 이미지 시각화\n","  # results 리스트의 위치 index가 바로 COCO 매핑된 Class id. 여기서는 result_ind가 class id\n","  # 개별 2차원 array에 오브젝트별 좌표와 class confidence score 값을 가짐.\n","  for result_ind, result in enumerate(results):\n","    # 개별 2차원 array의 row size가 0 이면 해당 Class id로 값이 없으므로 다음 loop로 진행.\n","    if len(result) == 0:\n","      continue\n","\n","    # 2차원 array에서 5번째 컬럼에 해당하는 값이 score threshold이며 이 값이 함수 인자로 들어온 score_threshold 보다 낮은 경우는 제외.\n","    result_filtered = result[np.where(result[:, 4] > score_threshold)]\n","\n","    # 해당 클래스 별로 Detect된 여러개의 오브젝트 정보가 2차원 array에 담겨 있으며, 이 2차원 array를 row수만큼 iteration해서 개별 오브젝트의 좌표값 추출.\n","    for i in range(len(result_filtered)):\n","      # 좌상단, 우하단 좌표 추출.\n","      left = int(result_filtered[i, 0])\n","      top = int(result_filtered[i, 1])\n","      right = int(result_filtered[i, 2])\n","      bottom = int(result_filtered[i, 3])\n","      caption = \"{}: {:.4f}\".format(labels_to_names_seq[result_ind], result_filtered[i, 4])\n","      cv2.rectangle(draw_img, (left, top), (right, bottom), color=bbox_color, thickness=2)\n","      cv2.putText(draw_img, caption, (int(left), int(top - 7)), cv2.FONT_HERSHEY_SIMPLEX, 0.6, text_color, 1)\n","      if is_print:\n","        print(caption)\n","\n","  return draw_img"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iAGGvTGzXQdT"},"source":["import matplotlib.pyplot as plt\n","\n","img_arr = cv2.imread('/content/data/images/Abyssinian_88.jpg')\n","detected_img = get_detected_img(model, img_arr,  score_threshold=0.3, is_print=True)\n","# detect 입력된 이미지는 bgr임. 이를 최종 출력시 rgb로 변환\n","detected_img = cv2.cvtColor(detected_img, cv2.COLOR_BGR2RGB)\n","\n","plt.figure(figsize=(12, 12))\n","plt.imshow(detected_img)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hBknxlSqXb6x"},"source":["import matplotlib.pyplot as plt\n","import cv2\n","%matplotlib inline\n","\n","def show_detected_images(model, img_arrays, ncols=5):\n","    figure, axs = plt.subplots(figsize=(22, 6), nrows=1, ncols=ncols)\n","    for i in range(ncols):\n","      detected_img = get_detected_img(model, img_arrays[i],  score_threshold=0.5, is_print=True)\n","      detected_img = cv2.cvtColor(detected_img, cv2.COLOR_BGR2RGB)\n","      #detected_img = cv2.resize(detected_img, (328, 328))\n","      axs[i].imshow(detected_img)\n","\n","\n","show_detected_images(model_ckpt, val_imgs[:5], ncols=5)\n","show_detected_images(model_ckpt, val_imgs[5:10], ncols=5)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qmlQPkqZZfPK"},"source":["val_paths = val_df[val_df['img_path'].str.contains('Persian')]['img_path'].values\n","val_imgs = [cv2.imread(x) for x in val_paths]\n","\n","show_detected_images(model_ckpt, val_imgs[:5], ncols=5)\n","show_detected_images(model_ckpt, val_imgs[5:10], ncols=5)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LMPVC9S2av1g"},"source":[],"execution_count":null,"outputs":[]}]}
{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### MMDetection 설치\n* 강의 영상에는 pip install mmcv-full로 mmcv를 설치(약 10분 정도의 시간이 소요)\n* 실습코드는 pip install mmcv-full -f https://download.openmmlab.com/mmcv/dist/cu116/torch1.13/index.html 로 변경(설치에 12초 정도 걸림. 2022.09).\n*  2023년 4월 6일 기준으로 mmdetection이 3.0으로 upgrade됨. 실습 코드는 mmdetection 2.x 기준으므로 mmdetection 2.x 소스코드 설치 필요.\n* 2024년 9월 colab의 numpy version이 1.24로 upgrade되면서 일부 코드가 동작오류. numpy 1.23 으로 downgrade 적용\n* 2025년 1월 17일 Colab의 python 버전이 3.10에서 3.11로 버전업 되면서 pytorch 2.0, torchvision 0.15로 변경. mmcv도 !pip install mmcv-full -f https://download.openmmlab.com/mmcv/dist/cu118/torch2.0.0/index.html 로 변경.\n* 2025년 8월 25일 Colab의 python 버전이 3.11에서 3.12로 버전업 되면서 더 이상 mmcv-full이 제대로 설치 되지 않음.\n* 이에 Colab 환경에서 Kaggle 환경으로 실습환경 이관. Kaggle은 여전히 python 버전이 3.11임.\n* 기존 Colab의 디렉토리 구조는 /content 디렉토리를 기반으로 실습코드 수행됨. Kaggle에서는 /kaggle/working이며 자동으로 현재디렉토리(.)로 실습 코드를 변경함\n* 2025년 8월 25일 download.openmmlab.com 사이트의 ssl 이슈로 pip install 에 --trusted-host 옵션 및 wget에 --no-check-certificate 옵션 추가","metadata":{}},{"cell_type":"markdown","source":"#### pytorch, torchvision 다운그레이드","metadata":{}},{"cell_type":"code","source":"import torch\nprint(torch.__version__)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# pytorch 버전을 2.0으로 downgrade\n!pip install torch==2.0.0 torchvision==0.15.1 --index-url https://download.pytorch.org/whl/cu118","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### mmcv-full 설치\n* mmcv-full은 1.7.2 버전으로 설치. ssl 인증 이슈로 --trusted-host 옵션 추가","metadata":{}},{"cell_type":"code","source":"!pip install mmcv-full --trusted-host download.openmmlab.com -f https://download.openmmlab.com/mmcv/dist/cu118/torch2.0/index.html","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### MMDetection 2.x 버전 설치\n* 2023년 4월 6일 기준으로 mmdetection이 3.0으로 upgrade됨. 실습 코드는 mmdetection 2.x 기준임.\n* mmdetection 2.x branch의 소스코드 기반으로 mmdetection 설치 필요.","metadata":{}},{"cell_type":"code","source":"# 2023년 4월 6일 기준으로 mmdetection이 3.0으로 upgrade됨. 실습 코드는 mmdetection 2.x 기준임.\n# mmdetection 2.x branch의 소스코드 기반으로 mmdetection 설치 필요.\n!git clone --branch 2.x https://github.com/open-mmlab/mmdetection.git\n!cd mmdetection; python setup.py install","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### mmdetection 2.x 버전에서 numpy 호환성 이슈로 numpy downgrade\n* 반드시 numpy downgrade를 mmcv 설치 후에 실행할것.","metadata":{}},{"cell_type":"code","source":"### 반드시 numpy downgrade를 mmcv 설치 후에 실행할것.\n!pip install numpy==1.23","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### mmcv와 mmdetection이 제대로 설치되었는지 확인 ","metadata":{}},{"cell_type":"code","source":"# 아래를 수행하기 전에 kernel을 restart 해야 함.\nfrom mmdet.apis import init_detector, inference_detector\nimport mmcv","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### KITTI Dataset 다운로드\n* 작은 용량의 KITTI Dataset을 다운로드하고 현재 디렉토리(/kaggle/working) 밑에 압축 해제\n* 강의 영상은 colab 기준. /content 디렉토리를 kaggle 기준으로 현재 디렉토리 /kaggle/working 로 전환\n* download.openmmlab.com 사이트의 ssl 이슈로 wget에 --no-check-certificate 추가","metadata":{}},{"cell_type":"code","source":"!wget --no-check-certificate https://download.openmmlab.com/mmdetection/data/kitti_tiny.zip\n!unzip kitti_tiny.zip > /dev/null","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport cv2\n\nimg = cv2.cvtColor(cv2.imread('/kaggle/working/kitti_tiny/training/image_2/000068.jpeg'), cv2.COLOR_BGR2RGB)\nplt.figure(figsize=(15, 10))\nplt.imshow(img)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### KITTI DATA FORMAT 확인\n* 첫번째 컬럼은 해당 오브젝트의 클래스 명.\n* 5번째~8번째가 BOUNDING BOX 정보임. 좌상단(xmin, ymin), 우하단(xmax, ymax) 좌표 임.","metadata":{}},{"cell_type":"code","source":"!cat /kaggle/working/kitti_tiny/training/label_2/000068.txt","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 원본 kitti Dataset는 10개의 Class로 되어 있음. 'Car Van Truck Pedestrian Person_sitting Cyclist Tram Misc DontCare'\nCLASSES = ('Car', 'Truck', 'Pedestrian', 'Cyclist')\ncat2label = {k:i for i, k in enumerate(CLASSES)}\nprint(cat2label)\ncat2label['Car']","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"image_list = mmcv.list_from_file('/kaggle/working/kitti_tiny/train.txt')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"lines = mmcv.list_from_file('/kaggle/working/kitti_tiny/training/label_2/000064.txt')\n#print(lines)\ncontent = [line.strip().split(' ') for line in lines]\nbbox_names = [x[0] for x in content]\n#print(bbox_names)\nbboxes = [ [float(info) for info in x[4:8]] for x in content]\nprint(bboxes)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import copy\nimport os.path as osp\nimport cv2\n\nimport mmcv\nimport numpy as np\n\nfrom mmdet.datasets.builder import DATASETS\nfrom mmdet.datasets.custom import CustomDataset\n\n# 반드시 아래 Decorator 설정 할것.@DATASETS.register_module() 설정 시 force=True를 입력하지 않으면 Dataset 재등록 불가.\n@DATASETS.register_module(force=True)\nclass KittyTinyDataset(CustomDataset):\n  CLASSES = ('Car', 'Truck', 'Pedestrian', 'Cyclist')\n\n  ##### self.data_root: /content/kitti_tiny/ self.ann_file: /content/kitti_tiny/train.txt self.img_prefix: /content/kitti_tiny/training/image_2\n  #### ann_file: /content/kitti_tiny/train.txt\n  # annotation에 대한 모든 파일명을 가지고 있는 텍스트 파일을 __init__(self, ann_file)로 입력 받고, 이 self.ann_file이 load_annotations()의 인자로 입력\n  def load_annotations(self, ann_file):\n    print('##### self.data_root:', self.data_root, 'self.ann_file:', self.ann_file, 'self.img_prefix:', self.img_prefix)\n    print('#### ann_file:', ann_file)\n    cat2label = {k:i for i, k in enumerate(self.CLASSES)}\n    image_list = mmcv.list_from_file(self.ann_file)\n    # 포맷 중립 데이터를 담을 list 객체\n    data_infos = []\n\n    for image_id in image_list:\n      filename = '{0:}/{1:}.jpeg'.format(self.img_prefix, image_id)\n      # 원본 이미지의 너비, 높이를 image를 직접 로드하여 구함.\n      image = cv2.imread(filename)\n      height, width = image.shape[:2]\n      # 개별 image의 annotation 정보 저장용 Dict 생성. key값 filename 에는 image의 파일명만 들어감(디렉토리는 제외)\n      data_info = {'filename': str(image_id) + '.jpeg',\n                   'width': width, 'height': height}\n      # 개별 annotation이 있는 서브 디렉토리의 prefix 변환.\n      label_prefix = self.img_prefix.replace('image_2', 'label_2')\n      # 개별 annotation 파일을 1개 line 씩 읽어서 list 로드\n      lines = mmcv.list_from_file(osp.join(label_prefix, str(image_id)+'.txt'))\n\n      # 전체 lines를 개별 line별 공백 레벨로 parsing 하여 다시 list로 저장. content는 list의 list형태임.\n      # ann 정보는 numpy array로 저장되나 텍스트 처리나 데이터 가공이 list 가 편하므로 일차적으로 list로 변환 수행.\n      content = [line.strip().split(' ') for line in lines]\n      # 오브젝트의 클래스명은 bbox_names로 저장.\n      bbox_names = [x[0] for x in content]\n      # bbox 좌표를 저장\n      bboxes = [ [float(info) for info in x[4:8]] for x in content]\n\n      # 클래스명이 해당 사항이 없는 대상 Filtering out, 'DontCare'sms ignore로 별도 저장.\n      gt_bboxes = []\n      gt_labels = []\n      gt_bboxes_ignore = []\n      gt_labels_ignore = []\n\n      for bbox_name, bbox in zip(bbox_names, bboxes):\n        # 만약 bbox_name이 클래스명에 해당 되면, gt_bboxes와 gt_labels에 추가, 그렇지 않으면 gt_bboxes_ignore, gt_labels_ignore에 추가\n        if bbox_name in cat2label:\n          gt_bboxes.append(bbox)\n          # gt_labels에는 class id를 입력\n          gt_labels.append(cat2label[bbox_name])\n\n        else:\n          gt_bboxes_ignore.append(bbox)\n          gt_labels_ignore.append(-1)\n      # 개별 image별 annotation 정보를 가지는 Dict 생성. 해당 Dict의 value값은 모두 np.array임.\n      data_anno = {\n          'bboxes': np.array(gt_bboxes, dtype=np.float32).reshape(-1, 4),\n          'labels': np.array(gt_labels, dtype=np.long),\n          'bboxes_ignore': np.array(gt_bboxes_ignore, dtype=np.float32).reshape(-1, 4),\n          'labels_ignore': np.array(gt_labels_ignore, dtype=np.long)\n      }\n      # image에 대한 메타 정보를 가지는 data_info Dict에 'ann' key값으로 data_anno를 value로 저장.\n      data_info.update(ann=data_anno)\n      # 전체 annotation 파일들에 대한 정보를 가지는 data_infos에 data_info Dict를 추가\n      data_infos.append(data_info)\n\n    return data_infos\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"### Config 설정하고 Pretrained 모델 다운로드. 강의 영상 /content 디렉토리를 /kaggle/working으로 변경. \nconfig_file = '/kaggle/working/mmdetection/configs/faster_rcnn/faster_rcnn_r50_fpn_1x_coco.py'\ncheckpoint_file = '/kaggle/working/mmdetection/checkpoints/faster_rcnn_r50_fpn_1x_coco_20200130-047c8118.pth'","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 강의 영상 /content 디렉토리를 /kaggle/working으로 변경. \n!cd mmdetection; mkdir checkpoints\n!wget --no-check-certificate -O  /kaggle/working/mmdetection/checkpoints/faster_rcnn_r50_fpn_1x_coco_20200130-047c8118.pth http://download.openmmlab.com/mmdetection/v2.0/faster_rcnn/faster_rcnn_r50_fpn_1x_coco/faster_rcnn_r50_fpn_1x_coco_20200130-047c8118.pth","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from mmcv import Config\n\ncfg = Config.fromfile(config_file)\nprint(cfg.pretty_text)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from mmdet.apis import set_random_seed\nimport torch\n# dataset에 대한 환경 파라미터 수정.\ncfg.dataset_type = 'KittyTinyDataset'\ncfg.data_root = '/kaggle/working/kitti_tiny/'\n\n# train, val, test dataset에 대한 type, data_root, ann_file, img_prefix 환경 파라미터 수정.\ncfg.data.train.type = 'KittyTinyDataset'\ncfg.data.train.data_root = '/kaggle/working/kitti_tiny/'\ncfg.data.train.ann_file = 'train.txt'\ncfg.data.train.img_prefix = 'training/image_2'\n\ncfg.data.val.type = 'KittyTinyDataset'\ncfg.data.val.data_root = '/kaggle/working/kitti_tiny/'\ncfg.data.val.ann_file = 'val.txt'\ncfg.data.val.img_prefix = 'training/image_2'\n\ncfg.data.test.type = 'KittyTinyDataset'\ncfg.data.test.data_root = '/kaggle/working/kitti_tiny/'\ncfg.data.test.ann_file = 'val.txt'\ncfg.data.test.img_prefix = 'training/image_2'\n\n# class의 갯수 수정.\ncfg.model.roi_head.bbox_head.num_classes = 4\n# pretrained 모델\ncfg.load_from = 'checkpoints/faster_rcnn_r50_fpn_1x_coco_20200130-047c8118.pth'\n\n# 학습 weight 파일로 로그를 저장하기 위한 디렉토리 설정.\ncfg.work_dir = './tutorial_exps'\n\n# 학습율 변경 환경 파라미터 설정.\ncfg.optimizer.lr = 0.02 / 8\n\ncfg.lr_config.warmup = None\ncfg.log_config.interval = 10\n\n# config 수행 시마다 policy값이 없어지는 bug로 인하여 설정.\ncfg.lr_config.policy = 'step'\n\n# Change the evaluation metric since we use customized dataset.\ncfg.evaluation.metric = 'mAP'\n# We can set the evaluation interval to reduce the evaluation times\ncfg.evaluation.interval = 12\n# We can set the checkpoint saving interval to reduce the storage cost\ncfg.checkpoint_config.interval = 12\n\n# Set seed thus the results are more reproducible\ncfg.seed = 0\nset_random_seed(0, deterministic=False)\n# ConfigDict' object has no attribute 'device 오류 발생시 반드시 설정 필요. https://github.com/open-mmlab/mmdetection/issues/7901\ncfg.device = 'cuda'\ncfg.gpu_ids =range(1)\n\ncfg.lr_config = dict(\n    policy='step',       # required\n    step=[8, 11],        # epochs to decay\n    gamma=0.1,           # decay rate\n    by_epoch=True\n)\n\n# We can initialize the logger for training and have a look\n# at the final config used for training\nprint(f'Config:\\n{cfg.pretty_text}')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Config에서 설정한 Dataset과 Model, 동적 학습율, Pipeline 설정에 따라 모델 학습 수행.\n\n* train용 Dataset을 생성하고 이를 이용하여 학습 수행.","metadata":{}},{"cell_type":"code","source":"from mmdet.datasets import build_dataset\nfrom mmdet.models import build_detector\nfrom mmdet.apis import train_detector\n\n# train용 Dataset 생성.\ndatasets = [build_dataset(cfg.data.train)]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"datasets","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"datasets[0].CLASSES","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = build_detector(cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\nmodel.CLASSES = datasets[0].CLASSES","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 주의, config에 pretrained 모델 지정이 상대 경로로 설정됨 cfg.load_from = 'checkpoints/faster_rcnn_r50_fpn_1x_coco_20200130-047c8118.pth'\n# 아래와 같이 %cd mmdetection 지정 필요.\n\n%cd mmdetection\n\nmmcv.mkdir_or_exist(osp.abspath(cfg.work_dir))\n# epochs는 config의 runner 파라미터로 지정됨. 기본 12회\ntrain_detector(model, datasets, cfg, distributed=False, validate=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 학습된 model을 이용하여 inference 수행.\n* 강의 영상의 colab 경로 /content를 /kaggle/working으로 변경. ","metadata":{}},{"cell_type":"code","source":"from mmdet.apis import inference_detector, init_detector, show_result_pyplot\n\n# BGR Image 사용\nimg = cv2.imread('/kaggle/working/kitti_tiny/training/image_2/000068.jpeg')\n\nmodel.cfg = cfg\n\nresult = inference_detector(model, img)\nshow_result_pyplot(model, img, result)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 학습된 모델을 이용하여 Video Detection 수행하기","metadata":{}},{"cell_type":"code","source":"%cd /kaggle/working\n!mkdir /kaggle/working/data\n!wget -O /kaggle/working/data/the_rock_chase.mp4 https://github.com/chulminkw/DLCV/blob/master/data/video/the_rock_chase.mp4?raw=true","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"CLASSES = ('Car', 'Truck', 'Pedestrian', 'Cyclist')\nlabels_to_names_seq = {i:k for i, k in enumerate(CLASSES)}\nlabels_to_names_seq","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"CLASSES = ('Car', 'Truck', 'Pedestrian', 'Cyclist')\ncat2label = {k:i for i, k in enumerate(CLASSES)}\n\ndef get_detected_img(model, img_array,  score_threshold=0.3, is_print=True):\n  # 인자로 들어온 image_array를 복사.\n  draw_img = img_array.copy()\n  bbox_color=(0, 255, 0)\n  text_color=(0, 0, 255)\n\n  # model과 image array를 입력 인자로 inference detection 수행하고 결과를 results로 받음.\n  # results는 80개의 2차원 array(shape=(오브젝트갯수, 5))를 가지는 list.\n  results = inference_detector(model, img_array)\n\n  # 80개의 array원소를 가지는 results 리스트를 loop를 돌면서 개별 2차원 array들을 추출하고 이를 기반으로 이미지 시각화\n  # results 리스트의 위치 index가 바로 COCO 매핑된 Class id. 여기서는 result_ind가 class id\n  # 개별 2차원 array에 오브젝트별 좌표와 class confidence score 값을 가짐.\n  for result_ind, result in enumerate(results):\n    # 개별 2차원 array의 row size가 0 이면 해당 Class id로 값이 없으므로 다음 loop로 진행.\n    if len(result) == 0:\n      continue\n\n    # 2차원 array에서 5번째 컬럼에 해당하는 값이 score threshold이며 이 값이 함수 인자로 들어온 score_threshold 보다 낮은 경우는 제외.\n    result_filtered = result[np.where(result[:, 4] > score_threshold)]\n\n    # 해당 클래스 별로 Detect된 여러개의 오브젝트 정보가 2차원 array에 담겨 있으며, 이 2차원 array를 row수만큼 iteration해서 개별 오브젝트의 좌표값 추출.\n    for i in range(len(result_filtered)):\n      # 좌상단, 우하단 좌표 추출.\n      left = int(result_filtered[i, 0])\n      top = int(result_filtered[i, 1])\n      right = int(result_filtered[i, 2])\n      bottom = int(result_filtered[i, 3])\n      caption = \"{}: {:.4f}\".format(labels_to_names_seq[result_ind], result_filtered[i, 4])\n      cv2.rectangle(draw_img, (left, top), (right, bottom), color=bbox_color, thickness=2)\n      cv2.putText(draw_img, caption, (int(left), int(top - 7)), cv2.FONT_HERSHEY_SIMPLEX, 0.37, text_color, 1)\n      if is_print:\n        print(caption)\n\n  return draw_img","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import time\n\ndef do_detected_video(model, input_path, output_path, score_threshold, do_print=True):\n\n    cap = cv2.VideoCapture(input_path)\n\n    codec = cv2.VideoWriter_fourcc(*'XVID')\n\n    vid_size = (round(cap.get(cv2.CAP_PROP_FRAME_WIDTH)),round(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)))\n    vid_fps = cap.get(cv2.CAP_PROP_FPS)\n\n    vid_writer = cv2.VideoWriter(output_path, codec, vid_fps, vid_size)\n\n    frame_cnt = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n    print('총 Frame 갯수:', frame_cnt)\n    btime = time.time()\n    while True:\n        hasFrame, img_frame = cap.read()\n        if not hasFrame:\n            print('더 이상 처리할 frame이 없습니다.')\n            break\n        stime = time.time()\n        img_frame = get_detected_img(model, img_frame,  score_threshold=score_threshold, is_print=False)\n        if do_print:\n          print('frame별 detection 수행 시간:', round(time.time() - stime, 4))\n        vid_writer.write(img_frame)\n    # end of while loop\n\n    vid_writer.release()\n    cap.release()\n\n    print('최종 detection 완료 수행 시간:', round(time.time() - btime, 4))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# kaggle은 오른쪽 slide 메뉴에서 해당 파일 download 필요\ndo_detected_video(model, '/kaggle/working/data/the_rock_chase.mp4', '/kaggle/working/data/the_rock_chase_out1.mp4', score_threshold=0.4, do_print=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
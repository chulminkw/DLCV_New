{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[]},"accelerator":"GPU","kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### MMDetection 설치\n* 강의 영상에는 pip install mmcv-full로 mmcv를 설치(약 10분 정도의 시간이 소요)\n* 실습코드는 pip install mmcv-full -f https://download.openmmlab.com/mmcv/dist/cu116/torch1.13/index.html 로 변경(설치에 12초 정도 걸림. 2022.09).\n*  2023년 4월 6일 기준으로 mmdetection이 3.0으로 upgrade됨. 실습 코드는 mmdetection 2.x 기준으므로 mmdetection 2.x 소스코드 설치 필요.\n* 2024년 9월 colab의 numpy version이 1.24로 upgrade되면서 일부 코드가 동작오류. numpy 1.23 으로 downgrade 적용\n* 2025년 1월 17일 Colab의 python 버전이 3.10에서 3.11로 버전업 되면서 pytorch 2.0, torchvision 0.15로 변경. mmcv도 !pip install mmcv-full -f https://download.openmmlab.com/mmcv/dist/cu118/torch2.0.0/index.html 로 변경.\n* 2025년 8월 25일 Colab의 python 버전이 3.11에서 3.12로 버전업 되면서 더 이상 mmcv-full이 제대로 설치 되지 않음.\n* 이에 Colab 환경에서 Kaggle 환경으로 실습환경 이관. Kaggle은 여전히 python 버전이 3.11임.\n* 기존 Colab의 디렉토리 구조는 /content 디렉토리를 기반으로 실습코드 수행됨. Kaggle에서는 /kaggle/working이며 자동으로 현재디렉토리(.)로 실습 코드를 변경함\n* 2025년 8월 25일 download.openmmlab.com 사이트의 ssl 이슈로 pip install 에 --trusted-host 옵션 및 wget에 --no-check-certificate 옵션 추가","metadata":{"id":"Zr4A7LiVkNnv"}},{"cell_type":"markdown","source":"#### pytorch, torchvision 다운그레이드","metadata":{"id":"lM9HV1UUVPxa"}},{"cell_type":"code","source":"#코랩의 pytorch 버전이 2.x 로 upgrade\nimport torch\nprint(torch.__version__)","metadata":{"id":"rPlHbX7uQUa4","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# pytorch 버전을 2.0으로 downgrade\n!pip install torch==2.0.0 torchvision==0.15.1 --index-url https://download.pytorch.org/whl/cu118","metadata":{"id":"wT0CFXNXObby","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### mmcv-full 설치\n* mmcv-full은 1.7.2 버전으로 설치. ssl 인증 이슈로 --trusted-host 옵션 추가","metadata":{"id":"Py8pDKkWVWDe"}},{"cell_type":"code","source":"# mmdetection를 위해서 mmcv-full을 먼저 설치해야 함. https://mmcv.readthedocs.io/en/latest/get_started/installation.html 설치 과정 참조.\n!pip install mmcv-full --trusted-host download.openmmlab.com -f https://download.openmmlab.com/mmcv/dist/cu118/torch2.0/index.html","metadata":{"trusted":true,"id":"zdMSR63rkK0C"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### MMDetection 2.x 버전 설치\n* 2023년 4월 6일 기준으로 mmdetection이 3.0으로 upgrade됨. 실습 코드는 mmdetection 2.x 기준임.\n* mmdetection 2.x branch의 소스코드 기반으로 mmdetection 설치 필요.","metadata":{}},{"cell_type":"code","source":"# 2023년 4월 6일 기준으로 mmdetection이 3.0으로 upgrade됨. 실습 코드는 mmdetection 2.x 기준임.\n# mmdetection 2.x branch의 소스코드 기반으로 mmdetection 설치 필요.\n!git clone --branch 2.x https://github.com/open-mmlab/mmdetection.git\n!cd mmdetection; python setup.py install","metadata":{"id":"u16-5M2ca9Po","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### mmdetection 2.x 버전에서 numpy 호환성 이슈로 numpy downgrade\n* 반드시 numpy downgrade를 mmcv 설치 후에 실행할것.","metadata":{}},{"cell_type":"code","source":"### 반드시 numpy downgrade를 mmcv 설치 후에 실행할것.\n!pip install numpy==1.23","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### mmcv와 mmdetection이 제대로 설치되었는지 확인 ","metadata":{}},{"cell_type":"code","source":"# 아래를 수행하기 전에 kernel을 restart 해야 함.\nfrom mmdet.apis import init_detector, inference_detector\nimport mmcv","metadata":{"trusted":true,"id":"Chqi6PKikK0J"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Oxford Pet Dataset 다운로드\n* image와 annotations을 압축파일로 각각 download 수행.\n* 강의 영상은 colab 기준. /content 디렉토리를 kaggle 기준으로 현재 디렉토리 /kaggle/working 로 전환","metadata":{"id":"RbJUt3KCkK0M"}},{"cell_type":"code","source":"!wget https://www.robots.ox.ac.uk/~vgg/data/pets/data/images.tar.gz\n!wget https://www.robots.ox.ac.uk/~vgg/data/pets/data/annotations.tar.gz","metadata":{"trusted":true,"id":"qnNSafQgkK0O"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# /kaggle/working/data 디렉토리를 만들고 해당 디렉토리에 다운로드 받은 압축 파일 풀기.\n!mkdir /kaggle/working/data\n!tar -xvf images.tar.gz -C /kaggle/working/data\n!tar -xvf annotations.tar.gz -C /kaggle/working/data","metadata":{"trusted":true,"id":"NYIdnysnkK0P"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 이미지 디렉토리와 annotation 파일 살펴 보기","metadata":{"id":"468iC6rKkK0Q"}},{"cell_type":"code","source":"!ls -lia ./data/images/Abyss*.jpg","metadata":{"trusted":true,"id":"BgniG93SkK0R"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!ls -lia ./data/images","metadata":{"trusted":true,"id":"ErNR2x1TkK0S"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!cat ./data/annotations/xmls/Abyssinian_1.xml","metadata":{"trusted":true,"id":"WxDRuchokK0T"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import glob\nimport xml.etree.ElementTree as ET\n\n# annotation xml 파일 파싱해서 bbox정보 추출\ndef get_bboxes_from_xml_test(xml_file):\n  tree = ET.parse(xml_file)\n  root = tree.getroot()\n  bbox_names = []\n  bboxes = []\n  # 파일내에 있는 모든 object Element를 찾음.\n  for obj in root.findall('object'):\n\n    bbox_name = obj.find('name').text\n    xmlbox = obj.find('bndbox')\n    x1 = int(xmlbox.find('xmin').text)\n    y1 = int(xmlbox.find('ymin').text)\n    x2 = int(xmlbox.find('xmax').text)\n    y2 = int(xmlbox.find('ymax').text)\n\n    bbox_names.append(bbox_name)\n    bboxes.append([x1, y1, x2, y2])\n\n  return bbox_names, bboxes\n\nget_bboxes_from_xml_test('./data/annotations/xmls/Abyssinian_1.xml')","metadata":{"trusted":true,"id":"5kZxptFEkK0T"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!ls -lia ./data/annotations/xmls/Abys*.xml","metadata":{"trusted":true,"id":"hfRC8pIQkK0W"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### train, val image/annotation 메타 파일 보기\n* train과 valid 데이터로 나뉠 image와 annotation의 파일명을 가지는 메타 파일\n* train과 valid용 meta 파일을 별도로 만듬.","metadata":{"id":"z0S-4oXfkK0W"}},{"cell_type":"code","source":"!cd ./data/annotations; cat trainval.txt","metadata":{"trusted":true,"id":"9RAnxyE-kK0X"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\npet_df = pd.read_csv('./data/annotations/trainval.txt', sep=' ', header=None, names=['img_name', 'class_id', 'etc1', 'etc2'])\npet_df.head()","metadata":{"trusted":true,"id":"gsfIawuekK0Y"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pet_df['class_id'].value_counts()","metadata":{"trusted":true,"id":"DS5_q-EGkK0b"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pet_df['class_name'] = pet_df['img_name'].apply(lambda x:x[:x.rfind('_')])\npet_df.head()","metadata":{"trusted":true,"id":"c2kgnuVhkK0c"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ntrain_df, val_df = train_test_split(pet_df, test_size=0.1, stratify=pet_df['class_id'], random_state=2021)","metadata":{"trusted":true,"id":"chZFqtmZkK0d"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(train_df['class_id'].value_counts(), val_df['class_id'].value_counts())","metadata":{"trusted":true,"id":"up3CeUfbkK0e"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df = train_df.sort_values(by='img_name')\nval_df = val_df.sort_values(by='img_name')","metadata":{"trusted":true,"id":"Dtck1P1SkK0e"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ann_file로 주어지는 메타파일은 가급적이면 소스데이터의 가장 상단 디렉토리에 저장하는 것이 바람직.\ntrain_df['img_name'].to_csv('./data/train.txt', sep=' ', header=False, index=False)\nval_df['img_name'].to_csv('./data/val.txt', sep=' ', header=False, index=False)","metadata":{"trusted":true,"id":"-kSL6xVKkK0f"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df.head()","metadata":{"id":"BPuPir0UyYLU","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pet_classes_list = pet_df['class_name'].unique().tolist()\nprint(pet_classes_list)","metadata":{"trusted":true,"id":"0F0enZeBkK0f"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!echo 'train list #####'; cat ./data/train.txt","metadata":{"trusted":true,"id":"fcIZeeQvkK0f"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!echo ' valid list ###'; cat ./data/val.txt","metadata":{"trusted":true,"id":"OFtvaflTkK0g"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### mmdetection의 중립 annotation 포맷 변환\n* CLASSES 는 pet_df의 'class_name' 컬럼에 unique 데이터로 지정. class id는 tuple(list)형의 CLASSES의 index값에 따라 설정.\n* ann_file로 입력되는 메타 파일을 읽어서 개별 image정보와 ann 정보를 dict로 생성하여 data_infos list에 입력\n* 개별 XML 읽어서 ann 정보를 만드는 것은 get_bboxes_from_xml() 함수 이용.\n* 디버깅용으로 CustomDataset을 만들어서 미리 테스트 하는 방법도 고려.\n\n![mmdetection_anno.png](data:image/png;base64,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)","metadata":{"id":"CgwtsjEakK0g"}},{"cell_type":"code","source":"import xml.etree.ElementTree as ET\n\n# 1개의 annotation 파일에서 bbox 정보 추출. 여러개의 object가 있을 경우 이들 object의 name과 bbox 좌표들을 list로 반환.\ndef get_bboxes_from_xml(anno_dir, xml_file):\n  anno_xml_file = osp.join(anno_dir, xml_file)\n  tree = ET.parse(anno_xml_file)\n  root = tree.getroot()\n  bbox_names = []\n  bboxes = []\n\n  # 파일내에 있는 모든 object Element를 찾음.\n  for obj in root.findall('object'):\n    #obj.find('name').text는 cat 이나 dog을 반환\n    #bbox_name = obj.find('name').text\n    # object의 클래스명은 파일명에서 추출.\n    bbox_name = xml_file[:xml_file.rfind('_')]\n\n    xmlbox = obj.find('bndbox')\n    x1 = int(xmlbox.find('xmin').text)\n    y1 = int(xmlbox.find('ymin').text)\n    x2 = int(xmlbox.find('xmax').text)\n    y2 = int(xmlbox.find('ymax').text)\n\n    bboxes.append([x1, y1, x2, y2])\n    bbox_names.append(bbox_name)\n\n  return bbox_names, bboxes","metadata":{"trusted":true,"id":"I7G465eMkK0j"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import copy\nimport os.path as osp\n\nimport mmcv\nimport numpy as np\nimport cv2\n\nfrom mmdet.datasets.builder import DATASETS\nfrom mmdet.datasets.custom import CustomDataset\n\nimport xml.etree.ElementTree as ET\n\nPET_CLASSES = pet_df['class_name'].unique().tolist()\n\n@DATASETS.register_module(force=True)\nclass PetDataset(CustomDataset):\n  CLASSES = PET_CLASSES\n\n  # annotation에 대한 모든 파일명을 가지고 있는 텍스트 파일을 __init__(self, ann_file)로 입력 받고,\n  # 이 self.ann_file이 load_annotations()의 인자로 입력\n  def load_annotations(self, ann_file):\n    cat2label = {k:i for i, k in enumerate(self.CLASSES)}\n    image_list = mmcv.list_from_file(self.ann_file)\n    # 포맷 중립 데이터를 담을 list 객체\n    data_infos = []\n\n    for image_id in image_list:\n      # self.img_prefix는 images 가 입력될 것임.\n      filename = '{0:}/{1:}.jpg'.format(self.img_prefix, image_id)\n      # 원본 이미지의 너비, 높이를 image를 직접 로드하여 구함.\n      image = cv2.imread(filename)\n      height, width = image.shape[:2]\n      # 개별 image의 annotation 정보 저장용 Dict 생성. key값 filename에는 image의 파일명만 들어감(디렉토리는 제외)\n      # 영상에는 data_info = {'filename': filename 으로 되어 있으나 filename은 image 파일명만 들어가는게 맞음.\n      data_info = {'filename': str(image_id) + '.jpg',\n                  'width': width, 'height': height}\n      # 개별 annotation XML 파일이 있는 서브 디렉토리의 prefix 변환.\n      label_prefix = self.img_prefix.replace('images', 'annotations')\n\n      # 개별 annotation XML 파일을 1개 line 씩 읽어서 list 로드. annotation XML파일이 xmls 밑에 있음에 유의\n      anno_xml_file = osp.join(label_prefix, 'xmls/'+str(image_id)+'.xml')\n      # 메타 파일에는 이름이 있으나 실제로는 존재하지 않는 XML이 있으므로 이는 제외.\n      if not osp.exists(anno_xml_file):\n          continue\n\n      # get_bboxes_from_xml() 를 이용하여 개별 XML 파일에 있는 이미지의 모든 bbox 정보를 list 객체로 생성.\n      anno_dir = osp.join(label_prefix, 'xmls')\n      bbox_names, bboxes = get_bboxes_from_xml(anno_dir, str(image_id)+'.xml')\n      #print('#########:', bbox_names)\n\n      gt_bboxes = []\n      gt_labels = []\n      gt_bboxes_ignore = []\n      gt_labels_ignore = []\n\n      # bbox별 Object들의 class name을 class id로 매핑. class id는 tuple(list)형의 CLASSES의 index값에 따라 설정\n      for bbox_name, bbox in zip(bbox_names, bboxes):\n        # 만약 bbox_name이 클래스명에 해당 되면, gt_bboxes와 gt_labels에 추가, 그렇지 않으면 gt_bboxes_ignore, gt_labels_ignore에 추가\n        # bbox_name이 CLASSES중에 반드시 하나 있어야 함. 안 그러면 FILTERING 되므로 주의 할것.\n        if bbox_name in cat2label:\n            gt_bboxes.append(bbox)\n            # gt_labels에는 class id를 입력\n            gt_labels.append(cat2label[bbox_name])\n        else:\n            gt_bboxes_ignore.append(bbox)\n            gt_labels_ignore.append(-1)\n\n      # 개별 image별 annotation 정보를 가지는 Dict 생성. 해당 Dict의 value값을 np.array형태로 bbox의 좌표와 label값으로 생성.\n      data_anno = {\n        'bboxes': np.array(gt_bboxes, dtype=np.float32).reshape(-1, 4),\n        'labels': np.array(gt_labels, dtype=np.long),\n        'bboxes_ignore': np.array(gt_bboxes_ignore, dtype=np.float32).reshape(-1, 4),\n        'labels_ignore': np.array(gt_labels_ignore, dtype=np.long)\n      }\n\n      # image에 대한 메타 정보를 가지는 data_info Dict에 'ann' key값으로 data_anno를 value로 저장.\n      data_info.update(ann=data_anno)\n      # 전체 annotation 파일들에 대한 정보를 가지는 data_infos에 data_info Dict를 추가\n      data_infos.append(data_info)\n      #print(data_info)\n\n    return data_infos\n","metadata":{"trusted":true,"id":"Wh3h8h1ZkK0h"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 강의 영상은 colab 기준. /content 디렉토리를 kaggle 기준으로 현재 디렉토리 /kaggle/working 로 전환\nmmcv.list_from_file('/kaggle/working/data/train.txt')","metadata":{"id":"ZB2dSujvvjiV","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"''' 디버깅 용도로 CustomDataset을 흉내낸 클래스를 생성하여 다양한 테스트를 수행 가능 '''\n\nimport os.path as osp\n\nPET_CLASSES = pet_df['class_name'].unique().tolist()\n\n# 디버깅 용도로 CustomDataset을 흉내낸 클래스 생성.\nclass PetDataset_imsi():\n  CLASSES = PET_CLASSES\n\n  # 생성자 함수 생성.\n  def __init__(self, data_root, ann_file, img_prefix):\n      self.data_root = data_root\n      self.ann_file = osp.join(data_root, ann_file)\n      self.img_prefix = osp.join(data_root, img_prefix)\n\n      self.data_infos = self.load_annotations(self.ann_file)\n\n  # annotation에 대한 모든 파일명을 가지고 있는 텍스트 파일을 __init__(self, ann_file)로 입력 받고,\n  # 이 self.ann_file이 load_annotations()의 인자로 입력\n  def load_annotations(self, ann_file):\n    cat2label = {k:i for i, k in enumerate(self.CLASSES)}\n    image_list = mmcv.list_from_file(self.ann_file)\n    # 포맷 중립 데이터를 담을 list 객체\n    data_infos = []\n\n    for image_id in image_list:\n      # self.img_prefix는 images 가 입력될 것임.\n      filename = '{0:}/{1:}.jpg'.format(self.img_prefix, image_id)\n      # 원본 이미지의 너비, 높이를 image를 직접 로드하여 구함.\n      image = cv2.imread(filename)\n      height, width = image.shape[:2]\n      # 개별 image의 annotation 정보 저장용 Dict 생성. key값 filename에는 image의 파일명만 들어감(디렉토리는 제외)\n      # 영상에는 data_info = {'filename': filename 으로 되어 있으나 filename은 image 파일명만 들어가는게 맞음.\n      data_info = {'filename': str(image_id) + '.jpg',\n                  'width': width, 'height': height}\n      # 개별 annotation XML 파일이 있는 서브 디렉토리의 prefix 변환.\n      label_prefix = self.img_prefix.replace('images', 'annotations')\n\n      # 개별 annotation XML 파일을 1개 line 씩 읽어서 list 로드. annotation XML파일이 xmls 밑에 있음에 유의\n      anno_xml_file = osp.join(label_prefix, 'xmls/'+str(image_id)+'.xml')\n      # 메타 파일에는 이름이 있으나 실제로는 존재하지 않는 XML이 있으므로 이는 제외.\n      if not osp.exists(anno_xml_file):\n          continue\n\n      # get_bboxes_from_xml() 를 이용하여 개별 XML 파일에 있는 이미지의 모든 bbox 정보를 list 객체로 생성.\n      anno_dir = osp.join(label_prefix, 'xmls')\n      bbox_names, bboxes = get_bboxes_from_xml(anno_dir, str(image_id)+'.xml')\n      #print('#########:', bbox_names)\n\n      gt_bboxes = []\n      gt_labels = []\n      gt_bboxes_ignore = []\n      gt_labels_ignore = []\n\n      # bbox별 Object들의 class name을 class id로 매핑. class id는 tuple(list)형의 CLASSES의 index값에 따라 설정\n      for bbox_name, bbox in zip(bbox_names, bboxes):\n        # 만약 bbox_name이 클래스명에 해당 되면, gt_bboxes와 gt_labels에 추가, 그렇지 않으면 gt_bboxes_ignore, gt_labels_ignore에 추가\n        # bbox_name이 CLASSES중에 반드시 하나 있어야 함. 안 그러면 FILTERING 되므로 주의 할것.\n        if bbox_name in cat2label:\n            gt_bboxes.append(bbox)\n            # gt_labels에는 class id를 입력\n            gt_labels.append(cat2label[bbox_name])\n        else:\n            gt_bboxes_ignore.append(bbox)\n            gt_labels_ignore.append(-1)\n\n      # 개별 image별 annotation 정보를 가지는 Dict 생성. 해당 Dict의 value값을 np.array형태로 bbox의 좌표와 label값으로 생성.\n      data_anno = {\n        'bboxes': np.array(gt_bboxes, dtype=np.float32).reshape(-1, 4),\n        'labels': np.array(gt_labels, dtype=np.long),\n        'bboxes_ignore': np.array(gt_bboxes_ignore, dtype=np.float32).reshape(-1, 4),\n        'labels_ignore': np.array(gt_labels_ignore, dtype=np.long)\n      }\n\n      # image에 대한 메타 정보를 가지는 data_info Dict에 'ann' key값으로 data_anno를 value로 저장.\n      data_info.update(ann=data_anno)\n      # 전체 annotation 파일들에 대한 정보를 가지는 data_infos에 data_info Dict를 추가\n      data_infos.append(data_info)\n      #print(data_info)\n\n    return data_infos\n","metadata":{"id":"13JRMKBp1oLk","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 디버깅 용도로 생성한 클래스를 생성하고 data_infos를 10개만 추출하여 생성된 데이터 확인.\ntrain_ds = PetDataset_imsi(data_root='/kaggle/working/data', ann_file='train.txt', img_prefix='images')\nprint(train_ds.data_infos[:10])","metadata":{"trusted":true,"id":"deWhFR2LkK0j"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(train_ds.data_infos[:10])","metadata":{"id":"crit9vUFAl0G","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Config를 설정하고 COCO로 Pretrained 된 모델을 Download\n* config파일은 faster rcnn resnet 50 backbone 사용.\n* Oxford Pet 데이터는 학습에 시간에 소모 되므로 학습으로 생성된 모델을 Google Drive에 저장","metadata":{"id":"3XcTEPGikK0k"}},{"cell_type":"code","source":"config_file = './mmdetection/configs/faster_rcnn/faster_rcnn_r50_fpn_1x_coco.py'\ncheckpoint_file = './mmdetection/checkpoints/faster_rcnn_r50_fpn_1x_coco_20200130-047c8118.pth'","metadata":{"trusted":true,"id":"IB-Th_sXkK0l"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!cd mmdetection; mkdir checkpoints\n# download.openmmlab.com 사이트의 ssl 이슈로 pip install 에 --trusted-host 옵션 및 wget에 --no-check-certificate 옵션 추가\n!wget --no-check-certificate -O ./mmdetection/checkpoints/faster_rcnn_r50_fpn_1x_coco_20200130-047c8118.pth http://download.openmmlab.com/mmdetection/v2.0/faster_rcnn/faster_rcnn_r50_fpn_1x_coco/faster_rcnn_r50_fpn_1x_coco_20200130-047c8118.pth","metadata":{"trusted":true,"id":"vh8qg1i9kK0l"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from mmcv import Config\n\ncfg = Config.fromfile(config_file)\nprint(cfg.pretty_text)","metadata":{"trusted":true,"id":"lDynqedDkK0l"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Google Drive 연동은 kaggle에서 수행하지 않음(kaggle에서 기능 지원되지 않음)\n* 강의 영상의 google drive 연동은 Kaggle에서 수행하지 않으며 /kaggle/working 디렉토리에 학습된 checkpoint 파일 생성. ","metadata":{}},{"cell_type":"code","source":"# 강의 영상의 google drive 연동은 Kaggle에서 수행하지 않으며 /kaggle/working 디렉토리에 학습된 checkpoint 파일 생성. \n# # Google Drive 접근을 위한 Mount 적용.\n# import os, sys\n# from google.colab import drive\n\n# drive.mount('/content/gdrive')","metadata":{"id":"2PekQOUSqQZ3","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # soft link로 Google Drive Directory 연결.\n# !ln -s /content/gdrive/My\\ Drive/ /mydrive\n# !ls /mydrive","metadata":{"id":"xc2DjYj1KAce","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 강의 영상의 google drive 연동은 Kaggle에서 수행하지 않으며 /kaggle/working/pet_work_dir 디렉토리에 학습된 weight 파일 생성. \n# /kaggle/working 디렉토리 및에 생성. \n# !mkdir \"/mydrive/pet_work_dir\"\n!mkdir /kaggle/working/pet_work_dir","metadata":{"id":"NAqx-sQ6qbYI","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!nvidia-smi","metadata":{"id":"AtvWjy_pLaG_","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from mmdet.apis import set_random_seed\n\n# dataset에 대한 환경 파라미터 수정.\ncfg.dataset_type = 'PetDataset'\ncfg.data_root = '/kaggle/working/data/'\n\n# train, val, test dataset에 대한 type, data_root, ann_file, img_prefix 환경 파라미터 수정.\ncfg.data.train.type = 'PetDataset'\ncfg.data.train.data_root = '/kaggle/working/data/'\ncfg.data.train.ann_file = 'train.txt'\ncfg.data.train.img_prefix = 'images'\n\ncfg.data.val.type = 'PetDataset'\ncfg.data.val.data_root = '/kaggle/working/data/'\ncfg.data.val.ann_file = 'val.txt'\ncfg.data.val.img_prefix = 'images'\n\n# class의 갯수 수정.\ncfg.model.roi_head.bbox_head.num_classes = 37\n# pretrained 모델\ncfg.load_from = 'checkpoints/faster_rcnn_r50_fpn_1x_coco_20200130-047c8118.pth'\n\n# 강의 영상의 google drive 연동은 Kaggle에서 수행하지 않으며 /kaggle/working/pet_work_dir 디렉토리에 학습된 weight 파일 생성.\ncfg.work_dir = '/kaggle/working/pet_work_dir'\n\n# 학습율 변경 환경 파라미터 설정.\ncfg.optimizer.lr = 0.02 / 8\ncfg.lr_config.warmup = None\ncfg.log_config.interval = 5\n\ncfg.runner.max_epochs = 5\n\n# 평가 metric 설정.\ncfg.evaluation.metric = 'mAP'\n# 평가 metric 수행할 epoch interval 설정.\ncfg.evaluation.interval = 5\n# 학습 iteration시마다 모델을 저장할 epoch interval 설정.\ncfg.checkpoint_config.interval = 5\n\n# 학습 시 Batch size 설정(단일 GPU 별 Batch size로 설정됨)\ncfg.data.samples_per_gpu = 4\n\n# Set seed thus the results are more reproducible\ncfg.seed = 0\nset_random_seed(0, deterministic=False)\ncfg.gpu_ids = range(1)\n# 두번 config를 로드하면 lr_config의 policy가 사라지는 오류로 인하여 설정.\ncfg.lr_config.policy='step'\n\n# ConfigDict' object has no attribute 'device 오류 발생시 반드시 설정 필요. https://github.com/open-mmlab/mmdetection/issues/7901\ncfg.device='cuda'\n\n# We can initialize the logger for training and have a look\n# at the final config used for training\nprint(f'Config:\\n{cfg.pretty_text}')","metadata":{"trusted":true,"id":"yqNaYmsikK0m"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Train용 데이터를 생성하고 Oxford Dataset을 학습수행.\n* build_dataset()로 train config 설정에 따른 Train용 dataset 생성.\n* build_detector()로 train과 test config반영하여 model 생성.\n* train_detector()로 model 학습.  ","metadata":{"id":"o5aVbmLvlVOk"}},{"cell_type":"code","source":"from mmdet.datasets import build_dataset\nfrom mmdet.models import build_detector\nfrom mmdet.apis import train_detector\n\n# train용 Dataset 생성.\ndatasets = [build_dataset(cfg.data.train)]","metadata":{"trusted":true,"id":"c8eS_bH6kK0o"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"datasets","metadata":{"id":"OZIKIGNGdBhD","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%cd mmdetection\n\nmodel = build_detector(cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\nmodel.CLASSES = datasets[0].CLASSES\n\nmmcv.mkdir_or_exist(osp.abspath(cfg.work_dir))\n# epochs는 config의 runner 파라미터로 지정됨. 기본 12회\ntrain_detector(model, datasets, cfg, distributed=False, validate=True)","metadata":{"trusted":true,"id":"Oqdyf3aTkK0p"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!ls -lia /kaggle/working/data/images/english_cocker_spaniel*","metadata":{"id":"hvXNhkhNrwF2","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"id":"lr5U5cTQoXKa","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 학습된 model을 이용하여 inference 수행.\n* 현재 memory에서 학습된 model 및 checkpoint 기록된 model을 loading하여 inference 수행.\n* 강의 영상은 colab 기준. /content 디렉토리를 kaggle 기준으로 현재 디렉토리 /kaggle/working 로 전환","metadata":{"id":"t3KvDsjcs9qk"}},{"cell_type":"code","source":"from mmdet.apis import inference_detector, init_detector, show_result_pyplot\n\n# BGR Image 사용\nimg = cv2.imread('/kaggle/working/data/images/Abyssinian_88.jpg')\n\nmodel.cfg = cfg\n\nresult = inference_detector(model, img)\nshow_result_pyplot(model, img, result, score_thr=0.3)","metadata":{"id":"HXKprlp4s-3b","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"result","metadata":{"id":"lZRVOaW8-0JH","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 아래는 오류를 발생시킵니다. 현재 Customized학습된 모델의 inference시 image file로 인자가 주어졌을때 inference 오류 발생.\nimg_path = '/kaggle/working/data/images/Abyssinian_88.jpg'\n\nmodel.cfg = cfg\n\nresult = inference_detector(model, img_path)\nshow_result_pyplot(model, img, result, score_thr=0.3)","metadata":{"id":"dGpd5joetbw6","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Checkpoint 저장된 model 파일을 로딩하고 이를 이용하여 Inference 수행.","metadata":{"id":"5l6nVtPTO-cK"}},{"cell_type":"code","source":"from mmdet.apis import show_result_pyplot\n# 강의와 다르게 checkpoint 파일은 /kaggle/working/pet_work_dir 디렉토리에 생성됨. \ncheckpoint_file = '/kaggle/working/pet_work_dir/epoch_5.pth'\n\n# checkpoint 저장된 model 파일을 이용하여 모델을 생성, 이때 Config는 위에서 update된 config 사용.\nmodel_ckpt = init_detector(cfg, checkpoint_file, device='cuda:0')\n# BGR Image 사용\nimg = cv2.imread('/kaggle/working/data/images/Abyssinian_88.jpg')\n#model_ckpt.cfg = cfg\n\nresult = inference_detector(model_ckpt, img)\nshow_result_pyplot(model_ckpt, img, result, score_thr=0.3)","metadata":{"id":"oWB1nN1q8Vnp","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 여러개의 image들을 Inference 수행.\n* inference_detector(model, imgs) 에서 인자 imgs는 단일 이미지일 경우 string/array, 여러개의 이미지일 경우 list(string/array)를 입력\n* show_result_pyplot(model_ckpt, img, result, score_thr=0.3)는 여러개의 이미지를 한번에 나타내기 어려우므로 별도의 시각화 함수 get_detected_img()를 이용","metadata":{"id":"9XYuhliUcic0"}},{"cell_type":"code","source":"val_df","metadata":{"id":"2l_DYC09jqYj","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"val_df['img_path'] = '/kaggle/working/data/images/' + val_df['img_name'] + '.jpg'\nval_df.head()","metadata":{"id":"MTGHOZ1ShLgM","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"val_df[val_df['img_path'].str.contains('Abyssinian')]['img_path'].values","metadata":{"id":"AeJQlPKsj-0g","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"val_paths = val_df[val_df['img_path'].str.contains('Abyssinian')]['img_path'].values\nval_imgs = [cv2.imread(x) for x in val_paths]","metadata":{"id":"0UIvXnJCV0r9","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"type(val_imgs), len(val_imgs), val_imgs[0].shape","metadata":{"id":"dI5kpbT0WD5p","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 반환된 results는 개별 원소로 list를 가지는 list임.\nresults = inference_detector(model_ckpt, val_imgs)","metadata":{"id":"g-sQKKHZR8Fs","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(results), len(results[0]), results","metadata":{"id":"jt3bbMm1Tt3o","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"PET_CLASSES = pet_df['class_name'].unique().tolist()\nlabels_to_names_seq = {i:k for i, k in enumerate(PET_CLASSES)}\n\n# model과 원본 이미지 array, filtering할 기준 class confidence score를 인자로 가지는 inference 시각화용 함수 생성.\ndef get_detected_img(model, img_array,  score_threshold=0.3, is_print=True):\n  # 인자로 들어온 image_array를 복사.\n  draw_img = img_array.copy()\n  bbox_color=(0, 255, 0)\n  text_color=(0, 0, 255)\n\n  # model과 image array를 입력 인자로 inference detection 수행하고 결과를 results로 받음.\n  # results는 80개의 2차원 array(shape=(오브젝트갯수, 5))를 가지는 list.\n  results = inference_detector(model, img_array)\n\n  # 80개의 array원소를 가지는 results 리스트를 loop를 돌면서 개별 2차원 array들을 추출하고 이를 기반으로 이미지 시각화\n  # results 리스트의 위치 index가 바로 COCO 매핑된 Class id. 여기서는 result_ind가 class id\n  # 개별 2차원 array에 오브젝트별 좌표와 class confidence score 값을 가짐.\n  for result_ind, result in enumerate(results):\n    # 개별 2차원 array의 row size가 0 이면 해당 Class id로 값이 없으므로 다음 loop로 진행.\n    if len(result) == 0:\n      continue\n\n    # 2차원 array에서 5번째 컬럼에 해당하는 값이 score threshold이며 이 값이 함수 인자로 들어온 score_threshold 보다 낮은 경우는 제외.\n    result_filtered = result[np.where(result[:, 4] > score_threshold)]\n\n    # 해당 클래스 별로 Detect된 여러개의 오브젝트 정보가 2차원 array에 담겨 있으며, 이 2차원 array를 row수만큼 iteration해서 개별 오브젝트의 좌표값 추출.\n    for i in range(len(result_filtered)):\n      # 좌상단, 우하단 좌표 추출.\n      left = int(result_filtered[i, 0])\n      top = int(result_filtered[i, 1])\n      right = int(result_filtered[i, 2])\n      bottom = int(result_filtered[i, 3])\n      caption = \"{}: {:.4f}\".format(labels_to_names_seq[result_ind], result_filtered[i, 4])\n      cv2.rectangle(draw_img, (left, top), (right, bottom), color=bbox_color, thickness=2)\n      cv2.putText(draw_img, caption, (int(left), int(top - 7)), cv2.FONT_HERSHEY_SIMPLEX, 0.6, text_color, 1)\n      if is_print:\n        print(caption)\n\n  return draw_img","metadata":{"id":"6kDWfFi4-T-Q","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nimg_arr = cv2.imread('/kaggle/working/data/images/Abyssinian_88.jpg')\ndetected_img = get_detected_img(model, img_arr,  score_threshold=0.3, is_print=True)\n# detect 입력된 이미지는 bgr임. 이를 최종 출력시 rgb로 변환\ndetected_img = cv2.cvtColor(detected_img, cv2.COLOR_BGR2RGB)\n\nplt.figure(figsize=(12, 12))\nplt.imshow(detected_img)","metadata":{"id":"iAGGvTGzXQdT","trusted":true,"execution":{"execution_failed":"2025-08-25T09:03:51.639Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport cv2\n%matplotlib inline\n\ndef show_detected_images(model, img_arrays, ncols=5):\n    figure, axs = plt.subplots(figsize=(22, 6), nrows=1, ncols=ncols)\n    for i in range(ncols):\n      detected_img = get_detected_img(model, img_arrays[i],  score_threshold=0.5, is_print=True)\n      detected_img = cv2.cvtColor(detected_img, cv2.COLOR_BGR2RGB)\n      #detected_img = cv2.resize(detected_img, (328, 328))\n      axs[i].imshow(detected_img)\n\n\nshow_detected_images(model_ckpt, val_imgs[:5], ncols=5)\nshow_detected_images(model_ckpt, val_imgs[5:10], ncols=5)","metadata":{"id":"hBknxlSqXb6x","trusted":true,"execution":{"execution_failed":"2025-08-25T09:03:51.639Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"val_paths = val_df[val_df['img_path'].str.contains('Persian')]['img_path'].values\nval_imgs = [cv2.imread(x) for x in val_paths]\n\nshow_detected_images(model_ckpt, val_imgs[:5], ncols=5)\nshow_detected_images(model_ckpt, val_imgs[5:10], ncols=5)","metadata":{"id":"qmlQPkqZZfPK","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"id":"LMPVC9S2av1g","trusted":true},"outputs":[],"execution_count":null}]}
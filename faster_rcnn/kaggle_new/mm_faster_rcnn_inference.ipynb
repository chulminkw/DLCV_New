{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"authorship_tag":"ABX9TyPMDyhvHvP7Oz6yAYpyxrlj"},"accelerator":"GPU","kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### MMDetection 설치\n* 강의 영상에는 pip install mmcv-full로 mmcv를 설치(약 10분 정도의 시간이 소요)\n* 실습코드는 pip install mmcv-full -f https://download.openmmlab.com/mmcv/dist/cu116/torch1.13/index.html 로 변경(설치에 12초 정도 걸림. 2022.09).\n*  2023년 4월 6일 기준으로 mmdetection이 3.0으로 upgrade됨. 실습 코드는 mmdetection 2.x 기준으므로 mmdetection 2.x 소스코드 설치 필요.\n* 2024년 9월 colab의 numpy version이 1.24로 upgrade되면서 일부 코드가 동작오류. numpy 1.23 으로 downgrade 적용\n* 2025년 1월 17일 Colab의 python 버전이 3.10에서 3.11로 버전업 되면서 pytorch 2.0, torchvision 0.15로 변경. mmcv도 !pip install mmcv-full -f https://download.openmmlab.com/mmcv/dist/cu118/torch2.0.0/index.html 로 변경.\n* 2025년 8월 25일 Colab의 python 버전이 3.11에서 3.12로 버전업 되면서 더 이상 mmcv-full이 제대로 설치 되지 않음.\n* 이에 Colab 환경에서 Kaggle 환경으로 실습환경 이관. Kaggle은 여전히 python 버전이 3.11임.\n* 기존 Colab의 디렉토리 구조는 /content 디렉토리를 기반으로 실습코드 수행됨. Kaggle에서는 /kaggle/working이며 자동으로 현재디렉토리(.)로 실습 코드를 변경함\n* 2025년 8월 25일 download.openmmlab.com 사이트의 ssl 이슈로 pip install 에 --trusted-host 옵션 및 wget에 --no-check-certificate 옵션 추가","metadata":{"id":"TRvHk7ZOQb3D"}},{"cell_type":"markdown","source":"#### pytorch, torchvision 다운그레이드","metadata":{"id":"Y74XsvJDTUSZ"}},{"cell_type":"code","source":"import torch\nprint(torch.__version__)","metadata":{"id":"JbciaaGZV__9","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# pytorch 버전을 2.0으로 downgrade\n!pip install torch==2.0.0 torchvision==0.15.1 --index-url https://download.pytorch.org/whl/cu118","metadata":{"id":"DpsE9kPJV4eD","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### mmcv-full 설치\n* mmcv-full은 1.7.2 버전으로 설치. ssl 인증 이슈로 --trusted-host 옵션 추가","metadata":{"id":"8Ko0BgGLTaMF"}},{"cell_type":"code","source":"# mmdetection를 위해서 mmcv-full을 먼저 설치해야 함. https://mmcv.readthedocs.io/en/latest/get_started/installation.html 설치 과정 참조.\n!pip install mmcv-full --trusted-host download.openmmlab.com -f https://download.openmmlab.com/mmcv/dist/cu118/torch2.0/index.html","metadata":{"id":"b9CiAfaFbk-9","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### MMDetection 2.x 버전 설치\n* 2023년 4월 6일 기준으로 mmdetection이 3.0으로 upgrade됨. 실습 코드는 mmdetection 2.x 기준임.\n* mmdetection 2.x branch의 소스코드 기반으로 mmdetection 설치 필요.","metadata":{}},{"cell_type":"code","source":"# 2023년 4월 6일 기준으로 mmdetection이 3.0으로 upgrade됨. 실습 코드는 mmdetection 2.x 기준임.\n# mmdetection 2.x branch의 소스코드 기반으로 mmdetection 설치 필요.\n!git clone --branch 2.x https://github.com/open-mmlab/mmdetection.git\n!cd mmdetection; python setup.py install","metadata":{"id":"VLAewbdwbhM_","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### mmdetection 2.x 버전에서 numpy 호환성 이슈로 numpy downgrade\n* 반드시 numpy downgrade를 mmcv 설치 후에 실행할것.","metadata":{}},{"cell_type":"code","source":"### 반드시 numpy downgrade를 mmcv 설치 후에 실행할것.\n!pip install numpy==1.23","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### mmcv와 mmdetection이 제대로 설치되었는지 확인 ","metadata":{}},{"cell_type":"code","source":"# 아래를 수행하기 전에 kernel을 restart 해야 함.\nfrom mmdet.apis import init_detector, inference_detector\nimport mmcv","metadata":{"id":"mlIOlLLLjAP7","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### MS-COCO 데이터 기반으로 Faster RCNN Pretrained 모델을 활용하여 Inference 수행\n* Faster RCNN Pretrained 모델 다운로드\n* Faster RCNN용 Config 파일 설정.  \n* Inference 용 모델을 생성하고, Inference 적용\n* 강의 영상은 colab 기준. /content 디렉토리를 kaggle 기준으로 현재 디렉토리 /kaggle/working 로 전환\n* download.openmmlab.com 사이트의 ssl 이슈로 wget에 --no-check-certificate 추가","metadata":{"id":"qHm73mVCQmpc"}},{"cell_type":"code","source":"# pretrained weight 모델을 다운로드 받기 위해서 mmdetection/checkpoints 디렉토리를 만듬.\n!cd mmdetection; mkdir checkpoints","metadata":{"id":"Gs162gTppmTh","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 강의 영상은 colab 기준. /content 디렉토리를 kaggle 기준으로 현재 디렉토리 /kaggle/working 로 전환\n!wget --no-check-certificate -O /kaggle/working/mmdetection/checkpoints/faster_rcnn_r50_fpn_1x_coco_20200130-047c8118.pth http://download.openmmlab.com/mmdetection/v2.0/faster_rcnn/faster_rcnn_r50_fpn_1x_coco/faster_rcnn_r50_fpn_1x_coco_20200130-047c8118.pth","metadata":{"id":"BvBLA2ecqJsV","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!ls -lia /kaggle/working/mmdetection/checkpoints","metadata":{"id":"YbETJ0zYh_bf","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# config 파일을 설정하고, 다운로드 받은 pretrained 모델을 checkpoint로 설정.\nconfig_file = '/kaggle/working/mmdetection/configs/faster_rcnn/faster_rcnn_r50_fpn_1x_coco.py'\ncheckpoint_file = '/kaggle/working/mmdetection/checkpoints/faster_rcnn_r50_fpn_1x_coco_20200130-047c8118.pth'","metadata":{"id":"DHyGQYXpoM_u","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# config 파일과 pretrained 모델을 기반으로 Detector 모델을 생성.\nfrom mmdet.apis import init_detector, inference_detector\n\nmodel = init_detector(config_file, checkpoint_file, device='cuda:0')","metadata":{"id":"BUgqp_sOpPKN","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# mmdetection은 상대 경로를 인자로 주면 무조건 mmdetection 디렉토리를 기준으로 함.\n%cd mmdetection\n\nfrom mmdet.apis import init_detector, inference_detector\n\n# init_detector() 인자로 config와 checkpoint를 입력함.\nmodel = init_detector(config='configs/faster_rcnn/faster_rcnn_r50_fpn_1x_coco.py', checkpoint='checkpoints/faster_rcnn_r50_fpn_1x_coco_20200130-047c8118.pth')","metadata":{"id":"2Ok9vjFcy7NK","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 강의 영상은 colab 기준. /content 디렉토리를 kaggle 기준으로 현재 디렉토리 /kaggle/working 로 전환\n%cd /kaggle/working","metadata":{"id":"4-31iMTozQmD","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import cv2\nimport matplotlib.pyplot as plt\nimg = '/kaggle/working/mmdetection/demo/demo.jpg'\n\nimg_arr  = cv2.cvtColor(cv2.imread(img), cv2.COLOR_BGR2RGB)\nplt.figure(figsize=(12, 12))\nplt.imshow(img_arr)","metadata":{"id":"5DwPX9OslRgc","trusted":true,"execution":{"execution_failed":"2025-08-25T08:12:07.322Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"img = '/kaggle/working/mmdetection/demo/demo.jpg'\n# inference_detector의 인자로 string(file경로), ndarray가 단일 또는 list형태로 입력 될 수 있음.\nresults = inference_detector(model, img)","metadata":{"id":"1U7BfoXrpR4l","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"type(results), len(results)","metadata":{"id":"dkWvvhR6qvcZ","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# results는 list형으로 coco class의  0부터 79까지 class_id별로 80개의 array를 가짐.\n# 개별 array들은 각 클래스별로 5개의 값(좌표값과 class별로 confidence)을 가짐. 개별 class별로 여러개의 좌표를 가지면 여러개의 array가 생성됨.\n# 좌표는 좌상단(xmin, ymin), 우하단(xmax, ymax) 기준.\n# 개별 array의 shape는 (Detection된 object들의 수, 5(좌표와 confidence)) 임\n\nresults","metadata":{"id":"zzkO3S5osBgV","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"results[0].shape, results[1].shape, results[2].shape, results[3].shape","metadata":{"id":"DlHzIIj-q1p6","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from mmdet.apis import show_result_pyplot\n# inference 된 결과를 원본 이미지에 적용하여 새로운 image로 생성(bbox 처리된 image)\n# Default로 score threshold가 0.3 이상인 Object들만 시각화 적용. show_result_pyplot은 model.show_result()를 호출.\nshow_result_pyplot(model, img, results)","metadata":{"id":"vJDIEqGwq9wu","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Model의 Config 설정 확인하기","metadata":{"id":"BKKCgICT3fiR"}},{"cell_type":"code","source":"model.__dict__","metadata":{"id":"F1daKttM0f8a","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#print(model.cfg)\nprint(model.cfg.pretty_text)","metadata":{"id":"PAv-3gZI1Tlx","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### array를 inference_detector()에 입력할 경우에는 원본 array를 BGR 형태로 입력 필요(RGB 변환은 내부에서 수행하므로 BGR로 입력 필요)\n* 강의 영상의 colab 경로 /content를 /kaggle/working으로 변경.","metadata":{"id":"ZbQletok3lat"}},{"cell_type":"code","source":"import cv2\n\n# RGB가 아닌 BGR로 입력\nimg_arr = cv2.imread('/kaggle/working/mmdetection/demo/demo.jpg')\nresults = inference_detector(model, img_arr)\n\nshow_result_pyplot(model, img_arr, results)","metadata":{"id":"qJVKVJ8izkkR","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### show_result_pyplot()을 이용하지 않고, inference 결과를 image로 표현하기\n* model과 image array를 입력하면 해당 image를 detect하고 bbox 처리해주는 get_detected_img() 함수 생성.\n* COCO 클래스 매핑은 0 부터 순차적으로 적용됨.\n* results에 들어있는 array 값이 없는 경우는 해당 list의 index에 해당하는 class id값으로 object가 Detection되지 않은 것임.  \n* 개별 class의 score threshold가 낮은 값은 제외.\n* 강의 영상의 colab 경로 /content를 /kaggle/working으로 변경.","metadata":{"id":"XNWbL1aoQeHb"}},{"cell_type":"code","source":"# 0부터 순차적으로 클래스 매핑된 label 적용.\nlabels_to_names_seq = {0:'person',1:'bicycle',2:'car',3:'motorbike',4:'aeroplane',5:'bus',6:'train',7:'truck',8:'boat',9:'traffic light',10:'fire hydrant',\n                        11:'stop sign',12:'parking meter',13:'bench',14:'bird',15:'cat',16:'dog',17:'horse',18:'sheep',19:'cow',20:'elephant',\n                        21:'bear',22:'zebra',23:'giraffe',24:'backpack',25:'umbrella',26:'handbag',27:'tie',28:'suitcase',29:'frisbee',30:'skis',\n                        31:'snowboard',32:'sports ball',33:'kite',34:'baseball bat',35:'baseball glove',36:'skateboard',37:'surfboard',38:'tennis racket',39:'bottle',40:'wine glass',\n                        41:'cup',42:'fork',43:'knife',44:'spoon',45:'bowl',46:'banana',47:'apple',48:'sandwich',49:'orange',50:'broccoli',\n                        51:'carrot',52:'hot dog',53:'pizza',54:'donut',55:'cake',56:'chair',57:'sofa',58:'pottedplant',59:'bed',60:'diningtable',\n                        61:'toilet',62:'tvmonitor',63:'laptop',64:'mouse',65:'remote',66:'keyboard',67:'cell phone',68:'microwave',69:'oven',70:'toaster',\n                        71:'sink',72:'refrigerator',73:'book',74:'clock',75:'vase',76:'scissors',77:'teddy bear',78:'hair drier',79:'toothbrush' }\n\nlabels_to_names = {1:'person',2:'bicycle',3:'car',4:'motorcycle',5:'airplane',6:'bus',7:'train',8:'truck',9:'boat',10:'traffic light',\n                    11:'fire hydrant',12:'street sign',13:'stop sign',14:'parking meter',15:'bench',16:'bird',17:'cat',18:'dog',19:'horse',20:'sheep',\n                    21:'cow',22:'elephant',23:'bear',24:'zebra',25:'giraffe',26:'hat',27:'backpack',28:'umbrella',29:'shoe',30:'eye glasses',\n                    31:'handbag',32:'tie',33:'suitcase',34:'frisbee',35:'skis',36:'snowboard',37:'sports ball',38:'kite',39:'baseball bat',40:'baseball glove',\n                    41:'skateboard',42:'surfboard',43:'tennis racket',44:'bottle',45:'plate',46:'wine glass',47:'cup',48:'fork',49:'knife',50:'spoon',\n                    51:'bowl',52:'banana',53:'apple',54:'sandwich',55:'orange',56:'broccoli',57:'carrot',58:'hot dog',59:'pizza',60:'donut',\n                    61:'cake',62:'chair',63:'couch',64:'potted plant',65:'bed',66:'mirror',67:'dining table',68:'window',69:'desk',70:'toilet',\n                    71:'door',72:'tv',73:'laptop',74:'mouse',75:'remote',76:'keyboard',77:'cell phone',78:'microwave',79:'oven',80:'toaster',\n                    81:'sink',82:'refrigerator',83:'blender',84:'book',85:'clock',86:'vase',87:'scissors',88:'teddy bear',89:'hair drier',90:'toothbrush',\n                    91:'hair brush'}","metadata":{"id":"3BsbdPe9WFY6","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\n\n# np.where 사용법 예시.\narr1 = np.array([[3.75348572e+02, 1.19171005e+02, 3.81950867e+02, 1.34460617e+02,\n         1.35454759e-01],\n        [5.32362000e+02, 1.09554726e+02, 5.40526550e+02, 1.25222633e+02,\n         8.88786465e-01],\n        [3.61124298e+02, 1.09049202e+02, 3.68625610e+02, 1.22483063e+02,\n         7.20717013e-02]], dtype=np.float32)\nprint(arr1.shape)\n\narr1_filtered = arr1[np.where(arr1[:, 4] > 0.1)]\nprint('### arr1_filtered:', arr1_filtered, arr1_filtered.shape)","metadata":{"id":"WtZxYMcZX2X8","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"np.where(arr1[:, 4] > 0.1)","metadata":{"id":"Rhqh0gmM4vH_","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# model과 원본 이미지 array, filtering할 기준 class confidence score를 인자로 가지는 inference 시각화용 함수 생성.\ndef get_detected_img(model, img_array,  score_threshold=0.3, is_print=True):\n  # 인자로 들어온 image_array를 복사.\n  draw_img = img_array.copy()\n  bbox_color=(0, 255, 0)\n  text_color=(0, 0, 255)\n\n  # model과 image array를 입력 인자로 inference detection 수행하고 결과를 results로 받음.\n  # results는 80개의 2차원 array(shape=(오브젝트갯수, 5))를 가지는 list.\n  results = inference_detector(model, img_array)\n\n  # 80개의 array원소를 가지는 results 리스트를 loop를 돌면서 개별 2차원 array들을 추출하고 이를 기반으로 이미지 시각화\n  # results 리스트의 위치 index가 바로 COCO 매핑된 Class id. 여기서는 result_ind가 class id\n  # 개별 2차원 array에 오브젝트별 좌표와 class confidence score 값을 가짐.\n  for result_ind, result in enumerate(results):\n    # 개별 2차원 array의 row size가 0 이면 해당 Class id로 값이 없으므로 다음 loop로 진행.\n    if len(result) == 0:\n      continue\n\n    # 2차원 array에서 5번째 컬럼에 해당하는 값이 score threshold이며 이 값이 함수 인자로 들어온 score_threshold 보다 낮은 경우는 제외.\n    result_filtered = result[np.where(result[:, 4] > score_threshold)]\n\n    # 해당 클래스 별로 Detect된 여러개의 오브젝트 정보가 2차원 array에 담겨 있으며, 이 2차원 array를 row수만큼 iteration해서 개별 오브젝트의 좌표값 추출.\n    for i in range(len(result_filtered)):\n      # 좌상단, 우하단 좌표 추출.\n      left = int(result_filtered[i, 0])\n      top = int(result_filtered[i, 1])\n      right = int(result_filtered[i, 2])\n      bottom = int(result_filtered[i, 3])\n      caption = \"{}: {:.4f}\".format(labels_to_names_seq[result_ind], result_filtered[i, 4])\n      cv2.rectangle(draw_img, (left, top), (right, bottom), color=bbox_color, thickness=2)\n      cv2.putText(draw_img, caption, (int(left), int(top - 7)), cv2.FONT_HERSHEY_SIMPLEX, 0.37, text_color, 1)\n      if is_print:\n        print(caption)\n\n  return draw_img","metadata":{"id":"WG9q57paRkM1","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nimg_arr = cv2.imread('/kaggle/working/mmdetection/demo/demo.jpg')\ndetected_img = get_detected_img(model, img_arr,  score_threshold=0.3, is_print=True)\n# detect 입력된 이미지는 bgr임. 이를 최종 출력시 rgb로 변환\ndetected_img = cv2.cvtColor(detected_img, cv2.COLOR_BGR2RGB)\n\nplt.figure(figsize=(12, 12))\nplt.imshow(detected_img)","metadata":{"id":"s0DPDQ33mJMg","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#강의 영상의 colab 경로 /content를 /kaggle/working으로 변경.\n!mkdir /kaggle/working/data","metadata":{"id":"TvqsrQWcWmSH","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!wget -O /kaggle/working/data/beatles01.jpg https://raw.githubusercontent.com/chulminkw/DLCV/master/data/image/beatles01.jpg\n!ls -lia /kaggle/working/data/beatles01.jpg","metadata":{"id":"r8TEyDTWRkVf","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"img_arr = cv2.imread('/kaggle/working/data/beatles01.jpg')\ndetected_img = get_detected_img(model, img_arr,  score_threshold=0.5, is_print=True)\n# detect 입력된 이미지는 bgr임. 이를 최종 출력시 rgb로 변환\ndetected_img = cv2.cvtColor(detected_img, cv2.COLOR_BGR2RGB)\n\nplt.figure(figsize=(12, 12))\nplt.imshow(detected_img)","metadata":{"id":"M4bRWFILRkaK","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Video Inference 수행\n* mmdetection의 video_demo.py 대로 Video inference 수행시 image 처리 시간이 상대적으로 오래 걸림.\n* 이미지 처리 로직을 변경하여 적용\n* 강의 영상의 colab 경로 /content를 /kaggle/working으로 변경.","metadata":{"id":"3FraDdoVWw1y"}},{"cell_type":"code","source":"!wget -O /kaggle/working/data/John_Wick_small.mp4 https://github.com/chulminkw/DLCV/blob/master/data/video/John_Wick_small.mp4?raw=true","metadata":{"id":"00k77eI3045G","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from mmdet.apis import init_detector, inference_detector\nimport mmcv\n\nconfig_file = '/kaggle/working/mmdetection/configs/faster_rcnn/faster_rcnn_r50_fpn_1x_coco.py'\ncheckpoint_file = '/kaggle/working/mmdetection/checkpoints/faster_rcnn_r50_fpn_1x_coco_20200130-047c8118.pth'\nmodel = init_detector(config_file, checkpoint_file, device='cuda:0')","metadata":{"id":"ugo0hmp0YJ0D","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# https://github.com/open-mmlab/mmdetection/blob/master/demo/video_demo.py 대로 video detection 수행.\n\nimport cv2\n\nvideo_reader = mmcv.VideoReader('/kaggle/working/data/John_Wick_small.mp4')\nvideo_writer = None\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nvideo_writer = cv2.VideoWriter('/kaggle/working/data/John_Wick_small_out1.mp4', fourcc, video_reader.fps,(video_reader.width, video_reader.height))\n\nfor frame in mmcv.track_iter_progress(video_reader):\n  result = inference_detector(model, frame)\n  frame = model.show_result(frame, result, score_thr=0.4)\n\n  video_writer.write(frame)\n\nif video_writer:\n        video_writer.release()","metadata":{"id":"EzwaONdFcxaq","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Custom된 frame처리 로직을 적용하여 Video Inference 수행.\n* 기존에 사용한 get_detected_img()를 그대로 사용함.","metadata":{"id":"sJvWNaicLjrY"}},{"cell_type":"code","source":"# model과 원본 이미지 array, filtering할 기준 class confidence score를 인자로 가지는 inference 시각화용 함수 생성.\nimport numpy as np\n\n# 0부터 순차적으로 클래스 매핑된 label 적용.\nlabels_to_names_seq = {0:'person',1:'bicycle',2:'car',3:'motorbike',4:'aeroplane',5:'bus',6:'train',7:'truck',8:'boat',9:'traffic light',10:'fire hydrant',\n                        11:'stop sign',12:'parking meter',13:'bench',14:'bird',15:'cat',16:'dog',17:'horse',18:'sheep',19:'cow',20:'elephant',\n                        21:'bear',22:'zebra',23:'giraffe',24:'backpack',25:'umbrella',26:'handbag',27:'tie',28:'suitcase',29:'frisbee',30:'skis',\n                        31:'snowboard',32:'sports ball',33:'kite',34:'baseball bat',35:'baseball glove',36:'skateboard',37:'surfboard',38:'tennis racket',39:'bottle',40:'wine glass',\n                        41:'cup',42:'fork',43:'knife',44:'spoon',45:'bowl',46:'banana',47:'apple',48:'sandwich',49:'orange',50:'broccoli',\n                        51:'carrot',52:'hot dog',53:'pizza',54:'donut',55:'cake',56:'chair',57:'sofa',58:'pottedplant',59:'bed',60:'diningtable',\n                        61:'toilet',62:'tvmonitor',63:'laptop',64:'mouse',65:'remote',66:'keyboard',67:'cell phone',68:'microwave',69:'oven',70:'toaster',\n                        71:'sink',72:'refrigerator',73:'book',74:'clock',75:'vase',76:'scissors',77:'teddy bear',78:'hair drier',79:'toothbrush' }\n\ndef get_detected_img(model, img_array,  score_threshold=0.3, is_print=True):\n  # 인자로 들어온 image_array를 복사.\n  draw_img = img_array.copy()\n  bbox_color=(0, 255, 0)\n  text_color=(0, 0, 255)\n\n  # model과 image array를 입력 인자로 inference detection 수행하고 결과를 results로 받음.\n  # results는 80개의 2차원 array(shape=(오브젝트갯수, 5))를 가지는 list.\n  results = inference_detector(model, img_array)\n\n  # 80개의 array원소를 가지는 results 리스트를 loop를 돌면서 개별 2차원 array들을 추출하고 이를 기반으로 이미지 시각화\n  # results 리스트의 위치 index가 바로 COCO 매핑된 Class id. 여기서는 result_ind가 class id\n  # 개별 2차원 array에 오브젝트별 좌표와 class confidence score 값을 가짐.\n  for result_ind, result in enumerate(results):\n    # 개별 2차원 array의 row size가 0 이면 해당 Class id로 값이 없으므로 다음 loop로 진행.\n    if len(result) == 0:\n      continue\n\n    # 2차원 array에서 5번째 컬럼에 해당하는 값이 score threshold이며 이 값이 함수 인자로 들어온 score_threshold 보다 낮은 경우는 제외.\n    result_filtered = result[np.where(result[:, 4] > score_threshold)]\n\n    # 해당 클래스 별로 Detect된 여러개의 오브젝트 정보가 2차원 array에 담겨 있으며, 이 2차원 array를 row수만큼 iteration해서 개별 오브젝트의 좌표값 추출.\n    for i in range(len(result_filtered)):\n      # 좌상단, 우하단 좌표 추출.\n      left = int(result_filtered[i, 0])\n      top = int(result_filtered[i, 1])\n      right = int(result_filtered[i, 2])\n      bottom = int(result_filtered[i, 3])\n      caption = \"{}: {:.4f}\".format(labels_to_names_seq[result_ind], result_filtered[i, 4])\n      cv2.rectangle(draw_img, (left, top), (right, bottom), color=bbox_color, thickness=2)\n      cv2.putText(draw_img, caption, (int(left), int(top - 7)), cv2.FONT_HERSHEY_SIMPLEX, 0.37, text_color, 1)\n      if is_print:\n        print(caption)\n\n  return draw_img","metadata":{"id":"-EVLPJozgzUC","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import time\n\ndef do_detected_video(model, input_path, output_path, score_threshold, do_print=True):\n\n    cap = cv2.VideoCapture(input_path)\n\n    codec = cv2.VideoWriter_fourcc(*'XVID')\n\n    vid_size = (round(cap.get(cv2.CAP_PROP_FRAME_WIDTH)),round(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)))\n    vid_fps = cap.get(cv2.CAP_PROP_FPS)\n\n    vid_writer = cv2.VideoWriter(output_path, codec, vid_fps, vid_size)\n\n    frame_cnt = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n    print('총 Frame 갯수:', frame_cnt)\n    btime = time.time()\n    while True:\n        hasFrame, img_frame = cap.read()\n        if not hasFrame:\n            print('더 이상 처리할 frame이 없습니다.')\n            break\n        stime = time.time()\n        img_frame = get_detected_img(model, img_frame,  score_threshold=score_threshold, is_print=False)\n        if do_print:\n          print('frame별 detection 수행 시간:', round(time.time() - stime, 4))\n        vid_writer.write(img_frame)\n    # end of while loop\n\n    vid_writer.release()\n    cap.release()\n\n    print('최종 detection 완료 수행 시간:', round(time.time() - btime, 4))","metadata":{"id":"KLdfdrYQfPwF","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"do_detected_video(model, '/kaggle/working/data/John_Wick_small.mp4', '/kaggle/working/data/John_Wick_small_out2.mp4', score_threshold=0.4, do_print=True)","metadata":{"id":"ku0dJAsLhKtA","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"id":"RWzbe1ar1Plo","trusted":true},"outputs":[],"execution_count":null}]}